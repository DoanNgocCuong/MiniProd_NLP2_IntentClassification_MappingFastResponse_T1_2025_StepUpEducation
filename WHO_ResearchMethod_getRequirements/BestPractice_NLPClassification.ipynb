{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://github.com/Krish95-bg/DistilBERT-for-Intent-Detection-Conversation-Message-Classification-with-LLM/blob/main/Intent_Detection_In_Conversation_Messages_Using_Distill_Bert_For_Multi_Class_Classification.ipynb\n",
    "- DistilBERT for Intent Detection\n",
    "D·ª± √°n n√†y s·ª≠ d·ª•ng m√¥ h√¨nh DistilBERT ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc ƒë·ªÉ ph√¢n lo·∫°i √Ω ƒë·ªãnh c·ªßa ng∆∞·ªùi d√πng trong ng√¥n ng·ªØ t·ª± nhi√™n, ph√π h·ª£p cho c√°c ·ª©ng d·ª•ng nh∆∞ chatbot v√† tr·ª£ l√Ω ·∫£o."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê√¢y l√† **h∆∞·ªõng d·∫´n chi ti·∫øt c√°ch fine-tune m√¥ h√¨nh DistilBERT** tr√™n m·ªôt t·∫≠p d·ªØ li·ªáu tu·ª≥ ch·ªânh cho b√†i to√°n **ph√¢n lo·∫°i √Ω ƒë·ªãnh (Intent Classification)**. ƒê√¢y l√† m·ªôt b√†i to√°n ph·ªï bi·∫øn trong x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n (NLP), ƒë·∫∑c bi·ªát trong c√°c ·ª©ng d·ª•ng nh∆∞ chatbot ho·∫∑c h·ªá th·ªëng h·ªó tr·ª£ ·∫£o.\n",
    "\n",
    "---\n",
    "\n",
    "### **Ph√¢n t√≠ch t·ª´ng ph·∫ßn:**\n",
    "\n",
    "1. **C√†i ƒë·∫∑t th∆∞ vi·ªán:**\n",
    "   - S·ª≠ d·ª•ng c√°c th∆∞ vi·ªán nh∆∞:\n",
    "     - **`transformers`**: ƒê·ªÉ x·ª≠ l√Ω tokenization v√† m√¥ h√¨nh BERT (ho·∫∑c DistilBERT).\n",
    "     - **`datasets`**: ƒê·ªÉ qu·∫£n l√Ω v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu.\n",
    "     - **`sklearn`**: ƒê·ªÉ ƒë√°nh gi√° m√¥ h√¨nh (classification report, confusion matrix).\n",
    "\n",
    "2. **T·∫£i d·ªØ li·ªáu:**\n",
    "   - D·ªØ li·ªáu l√† m·ªôt t·∫≠p h·ª£p c√°c c√¢u (`text`) v√† nh√£n √Ω ƒë·ªãnh (`intent`) v·ªõi nhi·ªÅu l·ªõp nh∆∞: \n",
    "     - `playmusic`, `getweather`, `addtoplaylist`,...\n",
    "   - D·ªØ li·ªáu ƒë∆∞·ª£c chia th√†nh `text` v√† `intent`, v·ªõi `intent` l√† nh√£n ƒë·∫ßu ra cho b√†i to√°n ph√¢n lo·∫°i.\n",
    "\n",
    "3. **Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu:**\n",
    "   - Lo·∫°i b·ªè stopwords (t·ª´ kh√¥ng mang √Ω nghƒ©a nhi·ªÅu) b·∫±ng th∆∞ vi·ªán NLTK.\n",
    "   - Chuy·ªÉn nh√£n t·ª´ d·∫°ng vƒÉn b·∫£n (`intent`) sang d·∫°ng s·ªë (`Label`) ƒë·ªÉ d·ªÖ hu·∫•n luy·ªán.\n",
    "\n",
    "4. **Tokenization:**\n",
    "   - S·ª≠ d·ª•ng **DistilBERT Tokenizer** ƒë·ªÉ chuy·ªÉn ƒë·ªïi c√°c c√¢u th√†nh chu·ªói token v√† √°nh x·∫° token sang ID.\n",
    "   - Th√™m padding, truncation ƒë·ªÉ ƒë·∫£m b·∫£o c√°c c√¢u c√≥ ƒë·ªô d√†i nh·∫•t qu√°n.\n",
    "\n",
    "5. **Hu·∫•n luy·ªán m√¥ h√¨nh:**\n",
    "   - S·ª≠ d·ª•ng m√¥ h√¨nh **DistilBERTForSequenceClassification** t·ª´ th∆∞ vi·ªán HuggingFace.\n",
    "   - T√πy ch·ªânh s·ªë l∆∞·ª£ng nh√£n (7 nh√£n cho 7 l·ªõp √Ω ƒë·ªãnh).\n",
    "   - S·ª≠ d·ª•ng **Trainer API** ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi c√°c tham s·ªë:\n",
    "     - Batch size: 16 (hu·∫•n luy·ªán), 64 (ƒë√°nh gi√°).\n",
    "     - Epochs: 3.\n",
    "     - Learning rate: 2e-5.\n",
    "\n",
    "6. **ƒê√°nh gi√° m√¥ h√¨nh:**\n",
    "   - S·ª≠ d·ª•ng **classification report** v√† **confusion matrix** ƒë·ªÉ ƒë√°nh gi√° hi·ªáu qu·∫£ d·ª± ƒëo√°n tr√™n d·ªØ li·ªáu ki·ªÉm tra.\n",
    "   - K·∫øt qu·∫£ cho th·∫•y ƒë·ªô ch√≠nh x√°c r·∫•t cao (~99%), ch·ª©ng t·ªè m√¥ h√¨nh ph√π h·ª£p v·ªõi b√†i to√°n.\n",
    "\n",
    "7. **L∆∞u m√¥ h√¨nh:**\n",
    "   - L∆∞u m√¥ h√¨nh v√† tokenizer ƒë·ªÉ s·ª≠ d·ª•ng l·∫°i sau n√†y.\n",
    "   - ƒê√≥ng g√≥i v√† t·∫£i m√¥ h√¨nh v·ªÅ m√°y t√≠nh.\n",
    "\n",
    "8. **D·ª± ƒëo√°n √Ω ƒë·ªãnh:**\n",
    "   - X√¢y d·ª±ng h·ªá th·ªëng d·ª± ƒëo√°n:\n",
    "     - S·ª≠ d·ª•ng m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán ƒë·ªÉ d·ª± ƒëo√°n √Ω ƒë·ªãnh t·ª´ c√¢u nh·∫≠p v√†o.\n",
    "     - Hi·ªÉn th·ªã nh√£n √Ω ƒë·ªãnh t∆∞∆°ng ·ª©ng v·ªõi c√¢u nh·∫≠p.\n",
    "\n",
    "---\n",
    "\n",
    "### **√ù nghƒ©a c·ªßa d·ª± √°n:**\n",
    "D·ª± √°n n√†y minh h·ªça c√°ch:\n",
    "- **Fine-tune m√¥ h√¨nh pre-trained (DistilBERT)** cho b√†i to√°n NLP t√πy ch·ªânh.\n",
    "- X√¢y d·ª±ng quy tr√¨nh ƒë·∫ßy ƒë·ªß t·ª´ t·∫£i d·ªØ li·ªáu, ti·ªÅn x·ª≠ l√Ω, hu·∫•n luy·ªán, ƒë√°nh gi√°, l∆∞u m√¥ h√¨nh, v√† tri·ªÉn khai d·ª± ƒëo√°n.\n",
    "- ·ª®ng d·ª•ng c√°c k·ªπ thu·∫≠t hi·ªán ƒë·∫°i trong x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n.\n",
    "\n",
    "---\n",
    "\n",
    "N·∫øu b·∫°n c·∫ßn gi·∫£i th√≠ch th√™m ho·∫∑c h∆∞·ªõng d·∫´n c·ª• th·ªÉ h∆°n v·ªÅ m·ªôt ph·∫ßn n√†o trong quy tr√¨nh n√†y, h√£y cho m√¨nh bi·∫øt nh√©! üòä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. https://github.com/HelpRam/Fine-Tune-Pretrained-model-DistilBert-/blob/main/BERT_FineTuning_Student.ipynb\n",
    "- Fine-Tune Pretrained DistilBERT Model\n",
    "D·ª± √°n n√†y ch·ª©a m√£ ngu·ªìn ƒë·ªÉ fine-tune m√¥ h√¨nh DistilBERT ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc cho vi·ªác ph√°t hi·ªán √Ω ƒë·ªãnh trong c√°c tin nh·∫Øn h·ªôi tho·∫°i, bao g·ªìm c√°c b∆∞·ªõc t·ª´ chu·∫©n b·ªã d·ªØ li·ªáu ƒë·∫øn tri·ªÉn khai m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "ƒê√¢y l√† **h∆∞·ªõng d·∫´n chi ti·∫øt v√† v√≠ d·ª•** v·ªÅ c√°ch fine-tune m√¥ h√¨nh BERT (Bidirectional Encoder Representations from Transformers) cho b√†i to√°n **ph√¢n lo·∫°i vƒÉn b·∫£n**, c·ª• th·ªÉ l√† tr√™n b·ªô d·ªØ li·ªáu **CoLA (Corpus of Linguistic Acceptability)** t·ª´ b·ªô GLUE Benchmark.\n",
    "\n",
    "### √ù nghƒ©a t·ª´ng ph·∫ßn:\n",
    "1. **C√†i ƒë·∫∑t m√¥i tr∆∞·ªùng v√† th∆∞ vi·ªán**:\n",
    "   - C√†i c√°c th∆∞ vi·ªán nh∆∞ `torch`, `transformers`, `datasets`, l√† nh·ªØng th∆∞ vi·ªán c·∫ßn thi·∫øt ƒë·ªÉ l√†m vi·ªác v·ªõi m√¥ h√¨nh BERT v√† x·ª≠ l√Ω d·ªØ li·ªáu.\n",
    "\n",
    "2. **Ch·ªçn thi·∫øt b·ªã (Device Selection)**:\n",
    "   - Ki·ªÉm tra xem c√≥ GPU kh√¥ng v√† ch·ªçn thi·∫øt b·ªã GPU (n·∫øu c√≥) ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô hu·∫•n luy·ªán. N·∫øu kh√¥ng, s·∫Ω s·ª≠ d·ª•ng CPU.\n",
    "\n",
    "3. **T·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu**:\n",
    "   - B·ªô d·ªØ li·ªáu CoLA l√† m·ªôt t·∫≠p h·ª£p c√°c c√¢u ƒë∆∞·ª£c g√°n nh√£n l√† ƒë√∫ng ng·ªØ ph√°p (label = 1) ho·∫∑c sai ng·ªØ ph√°p (label = 0).\n",
    "   - D·ªØ li·ªáu ƒë∆∞·ª£c t·∫£i b·∫±ng th∆∞ vi·ªán `datasets` c·ªßa HuggingFace, v√† sau ƒë√≥ chuy·ªÉn th√†nh DataFrame ƒë·ªÉ d·ªÖ thao t√°c.\n",
    "\n",
    "4. **Tokenization**:\n",
    "   - Chuy·ªÉn ƒë·ªïi c√°c c√¢u th√†nh c√°c **token** (ƒë∆°n v·ªã nh·ªè nh·∫•t m√† m√¥ h√¨nh BERT c√≥ th·ªÉ hi·ªÉu), sau ƒë√≥ √°nh x·∫° ch√∫ng sang c√°c **ID s·ªë**.\n",
    "   - S·ª≠ d·ª•ng `BertTokenizer` c·ªßa m√¥ h√¨nh BERT ƒë·ªÉ th√™m c√°c token ƒë·∫∑c bi·ªát nh∆∞ `[CLS]`, `[SEP]`, v√† x·ª≠ l√Ω padding/truncating ƒë·ªÉ ƒë∆∞a c√°c c√¢u v·ªÅ c√πng ƒë·ªô d√†i.\n",
    "\n",
    "5. **T·∫°o DataLoader**:\n",
    "   - D·ªØ li·ªáu ƒë∆∞·ª£c chia th√†nh c√°c batch ƒë·ªÉ hu·∫•n luy·ªán v√† ki·ªÉm tra. Batch size ƒë∆∞·ª£c ch·ªçn d·ª±a tr√™n b√†i b√°o g·ªëc c·ªßa BERT.\n",
    "\n",
    "6. **Fine-tuning m√¥ h√¨nh BERT**:\n",
    "   - S·ª≠ d·ª•ng l·ªõp `BertForSequenceClassification` ƒë·ªÉ t√πy ch·ªânh BERT cho b√†i to√°n ph√¢n lo·∫°i.\n",
    "   - Th√™m m·ªôt l·ªõp ph√¢n lo·∫°i ƒë∆°n gi·∫£n (fully connected layer) l√™n tr√™n c√πng c·ªßa m√¥ h√¨nh BERT.\n",
    "   - S·ª≠ d·ª•ng c√°c h√†m loss v√† optimizer ph√π h·ª£p nh∆∞ `AdamW`.\n",
    "\n",
    "7. **Hu·∫•n luy·ªán v√† ƒë√°nh gi√°**:\n",
    "   - Chia d·ªØ li·ªáu th√†nh 90% ƒë·ªÉ hu·∫•n luy·ªán v√† 10% ƒë·ªÉ ki·ªÉm tra.\n",
    "   - Hu·∫•n luy·ªán qua nhi·ªÅu epoch, t√≠nh loss trung b√¨nh v√† ƒë·ªô ch√≠nh x√°c sau m·ªói epoch.\n",
    "   - S·ª≠ d·ª•ng Learning Rate Scheduler ƒë·ªÉ ƒëi·ªÅu ch·ªânh learning rate trong qu√° tr√¨nh hu·∫•n luy·ªán.\n",
    "\n",
    "8. **K·∫øt qu·∫£**:\n",
    "   - Sau khi hu·∫•n luy·ªán, hi·ªÉn th·ªã c√°c th√¥ng s·ªë nh∆∞ loss, accuracy, v√† th·ªùi gian cho t·ª´ng epoch.\n",
    "   - C√°c k·∫øt qu·∫£ ƒë∆∞·ª£c tr√¨nh b√†y d∆∞·ªõi d·∫°ng b·∫£ng ƒë·ªÉ d·ªÖ ph√¢n t√≠ch.\n",
    "\n",
    "### √ù nghƒ©a ch√≠nh:\n",
    "- H∆∞·ªõng d·∫´n n√†y minh h·ªça c√°ch t√πy ch·ªânh v√† fine-tune m√¥ h√¨nh BERT cho m·ªôt b√†i to√°n c·ª• th·ªÉ (ph√¢n lo·∫°i ng·ªØ ph√°p c√¢u).\n",
    "- N√≥ c≈©ng cung c·∫•p c√°ch s·ª≠ d·ª•ng th∆∞ vi·ªán HuggingFace v√† PyTorch ƒë·ªÉ th·ª±c hi·ªán to√†n b·ªô pipeline: t·ª´ t·∫£i d·ªØ li·ªáu, x·ª≠ l√Ω, ƒë·∫øn hu·∫•n luy·ªán m√¥ h√¨nh.\n",
    "\n",
    "N·∫øu b·∫°n ƒëang h·ªçc v·ªÅ Machine Learning ho·∫∑c NLP, ƒë√¢y l√† m·ªôt b√†i t·∫≠p r·∫•t t·ªët ƒë·ªÉ hi·ªÉu c√°ch ·ª©ng d·ª•ng c√°c m√¥ h√¨nh ng√¥n ng·ªØ ti√™n ti·∫øn nh∆∞ BERT. B·∫°n c·∫ßn th√™m h·ªó tr·ª£ ·ªü ph·∫ßn n√†o c·ª• th·ªÉ kh√¥ng? üòä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. https://github.com/Beckendrof/intent-classification/\n",
    "- Intent Classification with DistilBERT\n",
    "D·ª± √°n n√†y fine-tune m√¥ h√¨nh DistilBERT ƒë·ªÉ th·ª±c hi·ªán ph√¢n lo·∫°i √Ω ƒë·ªãnh ƒëa l·ªõp, v·ªõi t·∫≠p d·ªØ li·ªáu g·ªìm 84 l·ªõp c·ªßa c√°c ph·∫£n h·ªìi h·ªôi tho·∫°i chung. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Code a H√πng \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```python\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModel\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" ## Setup CUDA GPU 1\n",
    "\n",
    "\n",
    "class BERTIntentClassification(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, model_name=\"bert-base-uncased\", num_classes=10, dropout_rate=0.1, cache_dir = \"huggingface\"):\n",
    "        super(BERTIntentClassification, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name, cache_dir = cache_dir)\n",
    "        # Get BERT hidden size\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def freeze_bert(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def get_pooling(self, hidden_state, attention_mask):\n",
    "        \"\"\"\n",
    "        Get mean pooled representation from BERT hidden states\n",
    "        Args:\n",
    "            hidden_state: BERT output containing hidden states\n",
    "        Returns:\n",
    "            pooled_output: Mean pooled representation of the sequence\n",
    "        \"\"\"\n",
    "        # Get last hidden state\n",
    "        last_hidden_state = hidden_state.last_hidden_state  # Shape: [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            # Expand attention mask to match hidden state dimensions\n",
    "            attention_mask = attention_mask.unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
    "            \n",
    "            # Mask out padding tokens\n",
    "            masked_hidden = last_hidden_state * attention_mask\n",
    "            \n",
    "            # Calculate mean (sum / number of actual tokens)\n",
    "            sum_hidden = torch.sum(masked_hidden, dim=1)  # [batch_size, hidden_size]\n",
    "            count_tokens = torch.sum(attention_mask, dim=1)  # [batch_size, 1]\n",
    "            pooled_output = sum_hidden / count_tokens\n",
    "        else:\n",
    "            # If no attention mask, simply take mean of all tokens\n",
    "            pooled_output = torch.mean(last_hidden_state, dim=1)\n",
    "        \n",
    "        return pooled_output\n",
    "        \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        Args:\n",
    "            input_ids: Input token IDs\n",
    "            attention_mask: Attention mask for padding\n",
    "        Returns:\n",
    "            logits: Raw logits for each class\n",
    "        \"\"\"\n",
    "        # Get BERT hidden states\n",
    "        hidden_state = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        \n",
    "        # Get pooled representation\n",
    "        hidden_state_pooling = self.get_pooling(hidden_state=hidden_state, attention_mask=attention_mask)\n",
    "        \n",
    "        # Pass through FFNN classifier\n",
    "        logits = self.ffnn(hidden_state_pooling)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "class TrainerCustom(Trainer):\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        if \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        \n",
    "        # S·ª≠ d·ª•ng nn.CrossEntropyLoss() thay v√¨ nn.CrossEntropy\n",
    "        cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Ch·∫°y m√¥ h√¨nh v√† nh·∫≠n ƒë·∫ßu ra (logits)\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # ƒê·∫£m b·∫£o l·∫•y logits t·ª´ outputs (m√¥ h√¨nh tr·∫£ v·ªÅ tuple, l·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n l√† logits)\n",
    "        logits = outputs\n",
    "        \n",
    "        # T√≠nh to√°n loss\n",
    "        loss = cross_entropy_loss(logits, labels)\n",
    "        \n",
    "        # Tr·∫£ v·ªÅ loss v√† outputs n·∫øu c·∫ßn\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# B∆∞·ªõc 1: T·∫£i d·ªØ li·ªáu\n",
    "# S·ª≠ d·ª•ng dataset s·∫µn c√≥ t·ª´ Hugging Face ho·∫∑c t·∫£i t·ª´ file c·ª•c b·ªô\n",
    "dataset = load_dataset(\"imdb\", cache_dir = \"huggingface\")  # V√≠ d·ª•: D·ªØ li·ªáu IMDB ƒë·ªÉ ph√¢n lo·∫°i sentiment\n",
    "# Thay th·∫ø tr∆∞·ªùng 'text' th√†nh 'input_ids' trong train_dataset v√† test_dataset\n",
    "def preprocess_dataset(dataset):\n",
    "    return dataset.map(lambda example: {\n",
    "            \"input_ids\": example['text'],\n",
    "            \"label\": example['label']\n",
    "        }, \n",
    "        remove_columns=[\"text\"],\n",
    "        num_proc=4  # S·ª≠ d·ª•ng 4 ti·∫øn tr√¨nh song song ƒë·ªÉ x·ª≠ l√Ω nhanh h∆°n\n",
    "    )\n",
    "\n",
    "train_dataset = preprocess_dataset(dataset[\"train\"])\n",
    "test_dataset = preprocess_dataset(dataset[\"test\"])\n",
    "\n",
    "\n",
    "# B∆∞·ªõc 2: Chu·∫©n b·ªã tokenizer v√† token h√≥a d·ªØ li·ªáu\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir = \"huggingface\")\n",
    "model = BERTIntentClassification(\n",
    "    model_name=model_name,\n",
    "    num_classes=2\n",
    ")\n",
    "model.freeze_bert() # Froze Layer BERT\n",
    "max_seq_length = 512\n",
    "\n",
    "\n",
    "def collate_fn(features):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for element in features:\n",
    "        inputs.append(element.get(\"input_ids\"))\n",
    "        labels.append(element.get(\"label\"))\n",
    "    \n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    token_inputs = tokenizer(\n",
    "        inputs,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_seq_length,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=False,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    token_inputs.update({\n",
    "        \"labels\": labels,\n",
    "    })\n",
    "    return token_inputs\n",
    "\n",
    "# B∆∞·ªõc 6: C√†i ƒë·∫∑t tham s·ªë hu·∫•n luy·ªán\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£\n",
    "    eval_strategy=\"epoch\",    # ƒê√°nh gi√° sau m·ªói epoch\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    num_train_epochs=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=None,\n",
    "    logging_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",          # L∆∞u tr·ªçng s·ªë sau m·ªói epoch\n",
    "    save_total_limit=3,\n",
    ")\n",
    "\n",
    "# B∆∞·ªõc 7: T·∫°o Trainer\n",
    "trainer = TrainerCustom(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator = collate_fn,\n",
    ")\n",
    "\n",
    "# B∆∞·ªõc 8: Hu·∫•n luy·ªán\n",
    "trainer.train()\n",
    "\n",
    "# B∆∞·ªõc 9: ƒê√°nh gi√° tr√™n t·∫≠p ki·ªÉm tra\n",
    "trainer.evaluate()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So s√°nh : So s√°nh 2 code n√†y ??? response d·∫°ng b·∫£ng. C√°i n√†o: BEST PRACTICES H∆†N >??\n",
    "- Code a H√πng BERT v·ªõi code: Fine_Tunning_DistilBert https://github.com/HelpRam/Fine-Tune-Pretrained-model-DistilBert-/blob/main/Fine_Tunning_DistilBert_for_intention_Detection_.ipynb   v√† https://github.com/Krish95-bg/DistilBERT-for-Intent-Detection-Conversation-Message-Classification-with-LLM/blob/main/Intent_Detection_In_Conversation_Messages_Using_Distill_Bert_For_Multi_Class_Classification.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So s√°nh hai ƒëo·∫°n m√£ tr√™n\n",
    "| **Ti√™u ch√≠**                          | **ƒêo·∫°n m√£ 1 (DistilBERT Trainer)**                                                                                                                                                                                | **ƒêo·∫°n m√£ 2 (BERT Custom Trainer)**                                                                                                                                                                                                                                                                                                             | **BEST PRACTICES**                                |\n",
    "|---------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------|\n",
    "| **M√¥ h√¨nh s·ª≠ d·ª•ng**                   | S·ª≠ d·ª•ng **DistilBERTForSequenceClassification** t·ª´ th∆∞ vi·ªán HuggingFace.                                                                                                   | T·ª± x√¢y d·ª±ng m√¥ h√¨nh **BERTIntentClassification**, t√πy ch·ªânh l·ªõp FFNN, pooling, v√† dropout ƒë·ªÉ linh ho·∫°t h∆°n.                                                                                                                                                                                                                                     | **ƒêo·∫°n m√£ 2**: Linh ho·∫°t v√† t√πy ch·ªânh t·ªët h∆°n.   |\n",
    "| **T√≠ch h·ª£p HuggingFace Trainer API**  | S·ª≠ d·ª•ng `Trainer` API c√≥ s·∫µn, ƒë∆°n gi·∫£n h√≥a hu·∫•n luy·ªán v√† ƒë√°nh gi√°.                                                                                                         | T√πy ch·ªânh `TrainerCustom` v·ªõi h√†m `compute_loss` ri√™ng, ph√π h·ª£p cho c√°c b√†i to√°n ph·ª©c t·∫°p h∆°n.                                                                                                                                                                                                                                                   | **ƒêo·∫°n m√£ 2**: T√πy ch·ªânh linh ho·∫°t h∆°n.          |\n",
    "| **Tokenization v√† x·ª≠ l√Ω d·ªØ li·ªáu**     | S·ª≠ d·ª•ng `Dataset` t·ª´ HuggingFace v√† √°p d·ª•ng tokenization tr·ª±c ti·∫øp.                                                                                                       | Tokenization v·ªõi h√†m `collate_fn`, ƒë·∫£m b·∫£o c·∫•u tr√∫c ph√π h·ª£p cho c√°c t√°c v·ª• t√πy ch·ªânh.                                                                                                                                                                                                                                                             | **ƒêo·∫°n m√£ 2**: T·ªëi ∆∞u cho d·ªØ li·ªáu l·ªõn, c·∫•u tr√∫c t·ªët. |\n",
    "| **S·ª≠ d·ª•ng GPU (CUDA)**                | Kh√¥ng ch·ªâ ƒë·ªãnh c·ª• th·ªÉ thi·∫øt b·ªã.                                                                                                                                            | Ch·ªâ ƒë·ªãnh s·ª≠ d·ª•ng GPU c·ª• th·ªÉ (`os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"`) cho b√†i to√°n l·ªõn.                                                                                                                                                                                                                                                        | **ƒêo·∫°n m√£ 2**: Qu·∫£n l√Ω t√†i nguy√™n GPU t·ªët h∆°n.   |\n",
    "| **Freeze Layer BERT**                 | Kh√¥ng √°p d·ª•ng freezing cho c√°c l·ªõp BERT, t·∫•t c·∫£ c√°c tham s·ªë ƒë·ªÅu ƒë∆∞·ª£c hu·∫•n luy·ªán l·∫°i.                                                                                       | C√≥ th·ªÉ ƒë√≥ng bƒÉng c√°c l·ªõp BERT (`freeze_bert`) ƒë·ªÉ t·ªëi ∆∞u th·ªùi gian khi d·ªØ li·ªáu nh·ªè ho·∫∑c kh√¥ng c·∫ßn fine-tune to√†n b·ªô.                                                                                                                                                                                                                               | **ƒêo·∫°n m√£ 2**: T√πy ch·ªânh ph√π h·ª£p h∆°n.            |\n",
    "| **C√†i ƒë·∫∑t tham s·ªë hu·∫•n luy·ªán**        | S·ª≠ d·ª•ng tham s·ªë chu·∫©n v·ªõi **Trainer** (batch size, learning rate, epochs).                                                                                                 | S·ª≠ d·ª•ng tham s·ªë t√πy ch·ªânh cao h∆°n (batch size l·ªõn, epochs d√†i, logging chi ti·∫øt).                                                                                                                                                                                                                                                                 | **ƒêo·∫°n m√£ 2**: Ki·ªÉm so√°t t·ªët h∆°n quy tr√¨nh hu·∫•n luy·ªán. |\n",
    "| **X·ª≠ l√Ω d·ªØ li·ªáu song song**           | Kh√¥ng c√≥ x·ª≠ l√Ω song song.                                                                                                                                                  | S·ª≠ d·ª•ng `num_proc=4` ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu song song, tƒÉng t·ªëc tokenization v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu.                                                                                                                                                                                                                                                      | **ƒêo·∫°n m√£ 2**: TƒÉng t·ªëc x·ª≠ l√Ω d·ªØ li·ªáu.            |\n",
    "| **ƒê√°nh gi√° m√¥ h√¨nh**                  | S·ª≠ d·ª•ng `Trainer` ƒë·ªÉ th·ª±c hi·ªán ƒë√°nh gi√° t·ª± ƒë·ªông, bao g·ªìm classification report v√† confusion matrix.                                                                        | T·ª± thi·∫øt k·∫ø pipeline ƒë√°nh gi√°, linh ho·∫°t h∆°n cho b√†i to√°n t√πy ch·ªânh.                                                                                                                                                                                                                                                                              | **ƒêo·∫°n m√£ 2**: T√πy ch·ªânh t·ªët h∆°n.                |\n",
    "| **T·ªëc ƒë·ªô hu·∫•n luy·ªán**                 | Nhanh h∆°n do m√¥ h√¨nh DistilBERT nh·∫π h∆°n.                                                                                                                                   | Ch·∫≠m h∆°n v√¨ m√¥ h√¨nh BERT l·ªõn h∆°n v√† t√πy ch·ªânh chi ti·∫øt h∆°n.                                                                                                                                                                                                                                                                                       | **ƒêo·∫°n m√£ 1**: Nhanh h∆°n cho b√†i to√°n ƒë∆°n gi·∫£n.   |\n",
    "| **M·ª©c ƒë·ªô linh ho·∫°t**                  | H·∫°n ch·∫ø, ch·ªâ ph√π h·ª£p v·ªõi b√†i to√°n ƒë∆°n gi·∫£n do ph·ª• thu·ªôc nhi·ªÅu v√†o API c·ªßa HuggingFace.                                                                                     | Cao h∆°n, c√≥ th·ªÉ t√πy ch·ªânh cho c√°c b√†i to√°n ph·ª©c t·∫°p ho·∫∑c m√¥ h√¨nh ri√™ng bi·ªát.                                                                                                                                                                                                                                                                       | **ƒêo·∫°n m√£ 2**: Linh ho·∫°t h∆°n.                    |\n",
    "| **Kh·∫£ nƒÉng m·ªü r·ªông**                  | Ph·ª• thu·ªôc v√†o c·∫•u tr√∫c s·∫µn c√≥ c·ªßa HuggingFace `Trainer`.                                                                                                                   | Linh ho·∫°t, c√≥ th·ªÉ m·ªü r·ªông sang c√°c b√†i to√°n kh√°c nh∆∞ multi-label classification, regression, ho·∫∑c custom pooling.                                                                                                                                                                                                                                 | **ƒêo·∫°n m√£ 2**: M·ªü r·ªông t·ªët h∆°n.                  |\n",
    "\n",
    "---\n",
    "\n",
    "### **K·∫øt lu·∫≠n**\n",
    "- **ƒêo·∫°n m√£ 1 (DistilBERT Trainer)**: \n",
    "  - Ph√π h·ª£p cho c√°c b√†i to√°n ƒë∆°n gi·∫£n, n∆°i y√™u c·∫ßu tri·ªÉn khai nhanh ch√≥ng v√† kh√¥ng c·∫ßn nhi·ªÅu t√πy ch·ªânh.\n",
    "  - T·ªëi ∆∞u h√≥a t·ªët h∆°n v·ªÅ t·ªëc ƒë·ªô nh·ªù s·ª≠ d·ª•ng m√¥ h√¨nh DistilBERT (nh·∫π h∆°n).\n",
    "- **ƒêo·∫°n m√£ 2 (BERT Custom Trainer)**:\n",
    "  - T·ªët h∆°n cho b√†i to√°n ph·ª©c t·∫°p, y√™u c·∫ßu linh ho·∫°t v√† kh·∫£ nƒÉng t√πy ch·ªânh cao.\n",
    "  - H·ªó tr·ª£ c√°c m√¥ h√¨nh l·ªõn h∆°n v√† t·ªëi ∆∞u t·ªët h∆°n cho quy tr√¨nh hu·∫•n luy·ªán, ƒë√°nh gi√°.\n",
    "\n",
    "**BEST PRACTICES**: **ƒêo·∫°n m√£ 2** v√¨ kh·∫£ nƒÉng t√πy ch·ªânh, m·ªü r·ªông, v√† ki·ªÉm so√°t chi ti·∫øt t·ªët h∆°n."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
