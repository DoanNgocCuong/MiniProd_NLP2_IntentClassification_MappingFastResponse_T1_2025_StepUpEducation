{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLWoGrdRXTSg"
      },
      "source": [
        "## 1. T·∫£i D·ªØ Li·ªáu t·ª´ CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbMKcafkKCPk",
        "outputId": "5883c418-e437-4692-b33c-901e0fb32d78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zIVYywvyiq2L"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModel\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" ## Setup CUDA GPU 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Ki·ªÉm tra GPU kh·∫£ d·ª•ng\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "else:\n",
        "    print(\"No GPU found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SHnftkneP6B",
        "outputId": "8e2ddfd2-dd72-411e-c1ef-54308c1c9008"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Number of GPUs: 1\n",
            "GPU 0: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "def select_gpu():\n",
        "    \"\"\"\n",
        "    Ki·ªÉm tra GPU kh·∫£ d·ª•ng v√† t·ª± ƒë·ªông ch·ªçn GPU ph√π h·ª£p.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        num_gpus = torch.cuda.device_count()\n",
        "        print(f\"Number of GPUs available: {num_gpus}\")\n",
        "\n",
        "        # Duy·ªát qua c√°c GPU kh·∫£ d·ª•ng ƒë·ªÉ t√¨m GPU √≠t s·ª≠ d·ª•ng nh·∫•t\n",
        "        available_gpus = [torch.cuda.get_device_name(i) for i in range(num_gpus)]\n",
        "        print(\"Available GPUs:\", available_gpus)\n",
        "\n",
        "        for i in range(num_gpus):\n",
        "            try:\n",
        "                # ƒê·∫∑t GPU\n",
        "                os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(i)\n",
        "                device = torch.device(f\"cuda:{i}\")\n",
        "                torch.cuda.set_device(device)\n",
        "                print(f\"Using GPU: {torch.cuda.get_device_name(device.index)}\")\n",
        "                return device\n",
        "            except Exception as e:\n",
        "                print(f\"GPU {i} is not suitable: {e}\")\n",
        "\n",
        "        print(\"No suitable GPU found. Falling back to CPU.\")\n",
        "        return torch.device(\"cpu\")\n",
        "    else:\n",
        "        print(\"No GPUs available. Using CPU.\")\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "# T·ª± ƒë·ªông ch·ªçn GPU ho·∫∑c CPU\n",
        "device = select_gpu()\n",
        "\n",
        "# Ki·ªÉm tra l·∫°i thi·∫øt b·ªã ƒëang s·ª≠ d·ª•ng\n",
        "print(f\"Final selected device: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge41QgJadXLv",
        "outputId": "f9f3c814-ed41-41f7-c075-ddaf3506297d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of GPUs available: 1\n",
            "Available GPUs: ['Tesla T4']\n",
            "Using GPU: Tesla T4\n",
            "Final selected device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IkfPfDY3iskg"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BERTIntentClassification(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, model_name=\"bert-base-uncased\", num_classes=10, dropout_rate=0.1, cache_dir = \"huggingface\"):\n",
        "        super(BERTIntentClassification, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name, cache_dir = cache_dir)\n",
        "        # Get BERT hidden size\n",
        "        hidden_size = self.bert.config.hidden_size\n",
        "        self.ffnn = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.LayerNorm(hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size, num_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "    def freeze_bert(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "    def get_pooling(self, hidden_state, attention_mask):\n",
        "        \"\"\"\n",
        "        Get mean pooled representation from BERT hidden states\n",
        "        Args:\n",
        "            hidden_state: BERT output containing hidden states\n",
        "        Returns:\n",
        "            pooled_output: Mean pooled representation of the sequence\n",
        "        \"\"\"\n",
        "        # Get last hidden state\n",
        "        last_hidden_state = hidden_state.last_hidden_state  # Shape: [batch_size, seq_len, hidden_size]\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # Expand attention mask to match hidden state dimensions\n",
        "            attention_mask = attention_mask.unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
        "\n",
        "            # Mask out padding tokens\n",
        "            masked_hidden = last_hidden_state * attention_mask\n",
        "\n",
        "            # Calculate mean (sum / number of actual tokens)\n",
        "            sum_hidden = torch.sum(masked_hidden, dim=1)  # [batch_size, hidden_size]\n",
        "            count_tokens = torch.sum(attention_mask, dim=1)  # [batch_size, 1]\n",
        "            pooled_output = sum_hidden / count_tokens\n",
        "        else:\n",
        "            # If no attention mask, simply take mean of all tokens\n",
        "            pooled_output = torch.mean(last_hidden_state, dim=1)\n",
        "\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, **kwargs):\n",
        "        \"\"\"\n",
        "        Forward pass of the model\n",
        "        Args:\n",
        "            input_ids: Input token IDs\n",
        "            attention_mask: Attention mask for padding\n",
        "        Returns:\n",
        "            logits: Raw logits for each class\n",
        "        \"\"\"\n",
        "        # Get BERT hidden states\n",
        "        hidden_state = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        # Get pooled representation\n",
        "        hidden_state_pooling = self.get_pooling(hidden_state=hidden_state, attention_mask=attention_mask)\n",
        "\n",
        "        # Pass through FFNN classifier\n",
        "        logits = self.ffnn(hidden_state_pooling)\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zypDXvoaivAb"
      },
      "outputs": [],
      "source": [
        "class TrainerCustom(Trainer):\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        \"\"\"\n",
        "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
        "\n",
        "        Subclass and override for custom behavior.\n",
        "        \"\"\"\n",
        "        if \"labels\" in inputs:\n",
        "            labels = inputs.pop(\"labels\")\n",
        "        else:\n",
        "            labels = None\n",
        "\n",
        "        # S·ª≠ d·ª•ng nn.CrossEntropyLoss() thay v√¨ nn.CrossEntropy\n",
        "        cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Ch·∫°y m√¥ h√¨nh v√† nh·∫≠n ƒë·∫ßu ra (logits)\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # ƒê·∫£m b·∫£o l·∫•y logits t·ª´ outputs (m√¥ h√¨nh tr·∫£ v·ªÅ tuple, l·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n l√† logits)\n",
        "        logits = outputs\n",
        "\n",
        "        # T√≠nh to√°n loss\n",
        "        loss = cross_entropy_loss(logits, labels)\n",
        "\n",
        "        # Tr·∫£ v·ªÅ loss v√† outputs n·∫øu c·∫ßn\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zIJIYGcppMk"
      },
      "source": [
        "# 1. Load Dataset and with Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DQBY8_d5rwlj"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # B∆∞·ªõc 1: T·∫£i d·ªØ li·ªáu\n",
        "# # S·ª≠ d·ª•ng dataset s·∫µn c√≥ t·ª´ Hugging Face ho·∫∑c t·∫£i t·ª´ file c·ª•c b·ªô\n",
        "# dataset = load_dataset(\"imdb\", cache_dir = \"huggingface\")  # V√≠ d·ª•: D·ªØ li·ªáu IMDB ƒë·ªÉ ph√¢n lo·∫°i sentiment\n",
        "# # Thay th·∫ø tr∆∞·ªùng 'text' th√†nh 'input_ids' trong train_dataset v√† test_dataset\n",
        "# def preprocess_dataset(dataset):\n",
        "#     return dataset.map(lambda example: {\n",
        "#             \"input_ids\": example['text'],\n",
        "#             \"label\": example['label']\n",
        "#         },\n",
        "#         remove_columns=[\"text\"],\n",
        "#         num_proc=4  # S·ª≠ d·ª•ng 4 ti·∫øn tr√¨nh song song ƒë·ªÉ x·ª≠ l√Ω nhanh h∆°n\n",
        "#     )\n",
        "\n",
        "# train_dataset = preprocess_dataset(dataset[\"train\"])\n",
        "# test_dataset = preprocess_dataset(dataset[\"test\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RBMrLoabi9DJ"
      },
      "outputs": [],
      "source": [
        "# print(train_dataset)\n",
        "# # Truy c·∫≠p m·∫´u c·ª• th·ªÉ\n",
        "# train_sample = train_dataset[:10]\n",
        "# test_sample = test_dataset[:2]\n",
        "# print(train_sample)\n",
        "\n",
        "\n",
        "# from datasets import Dataset\n",
        "\n",
        "# train_sample = train_dataset[:10]\n",
        "\n",
        "# # Chuy·ªÉn t·ª´ dict v·ªÅ Dataset\n",
        "# train_sample_dataset = Dataset.from_dict(train_sample)\n",
        "# test_sample_dataset = Dataset.from_dict(test_sample)\n",
        "# print(train_sample_dataset)\n",
        "# print(type(train_sample_dataset))\n",
        "# # Output: <class 'datasets.arrow_dataset.Dataset'>\n",
        "\n",
        "\n",
        "# # In th·ª≠ 1 h√†ng trong test_sample_dataset\n",
        "# print(\"First row in test_sample_dataset:\")\n",
        "# print(test_sample_dataset[0])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "slaqKCOhjh9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab58ba35-662c-4a47-f96c-10ac6f51f3ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 27\n",
            "})\n",
            "Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 10\n",
            "})\n",
            "First row in test_sample_dataset:\n",
            "{'label': 'Agree', 'input_ids': 'Yes, I want to show you the picture.'}\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def load_csv_dataset(csv_path, text_column, label_column):\n",
        "    \"\"\"\n",
        "    T·∫£i dataset t·ª´ file CSV v√† ƒë·ªïi t√™n c·ªôt.\n",
        "\n",
        "    Args:\n",
        "        csv_path (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn file .csv.\n",
        "        text_column (str): T√™n c·ªôt ch·ª©a vƒÉn b·∫£n.\n",
        "        label_column (str): T√™n c·ªôt ch·ª©a nh√£n.\n",
        "\n",
        "    Returns:\n",
        "        Dataset: T·∫≠p d·ªØ li·ªáu ƒë√£ t·∫£i t·ª´ file .csv.\n",
        "    \"\"\"\n",
        "    # T·∫£i d·ªØ li·ªáu t·ª´ file .csv\n",
        "    dataset = Dataset.from_csv(csv_path)\n",
        "    # ƒê·ªïi t√™n c·ªôt\n",
        "    dataset = dataset.rename_columns({text_column: \"input_ids\", label_column: \"label\"})\n",
        "    return dataset\n",
        "\n",
        "# S·ª≠ d·ª•ng h√†m\n",
        "csv_path = \"/content/chatbot_intent_data_v1_En.csv\"             # ƒê∆∞·ªùng d·∫´n file CSV\n",
        "text_column = \"input_ids\"       # C·ªôt ch·ª©a vƒÉn b·∫£n\n",
        "label_column = \"label\"        # C·ªôt ch·ª©a nh√£n\n",
        "\n",
        "# T·∫£i dataset\n",
        "dataset = load_csv_dataset(csv_path, text_column, label_column)\n",
        "\n",
        "# Ki·ªÉm tra d·ªØ li·ªáu\n",
        "print(dataset)\n",
        "\n",
        "# Truy c·∫≠p m·∫´u c·ª• th·ªÉ\n",
        "sample_dataset = dataset.select(range(10))  # L·∫•y 10 m·∫´u ƒë·∫ßu ti√™n\n",
        "print(sample_dataset)\n",
        "\n",
        "\n",
        "# In th·ª≠ 1 h√†ng trong test_sample_dataset\n",
        "print(\"First row in test_sample_dataset:\")\n",
        "print(sample_dataset[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1oFci1j5k1UY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f5f8c9-831e-472f-c77b-24a531369faa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Invalid Samples =====\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "def check_invalid_samples(dataset):\n",
        "    invalid_samples = []\n",
        "    for idx, sample in enumerate(dataset):\n",
        "        if not isinstance(sample[\"input_ids\"], str) or sample[\"input_ids\"].strip() == \"\":\n",
        "            invalid_samples.append((idx, sample))\n",
        "    return invalid_samples\n",
        "\n",
        "# Ki·ªÉm tra d·ªØ li·ªáu kh√¥ng h·ª£p l·ªá\n",
        "invalid_samples = check_invalid_samples(dataset)\n",
        "print(\"\\n===== Invalid Samples =====\")\n",
        "print(invalid_samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "x0-3V7dLlCZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d136919-e74a-442d-c859-d6e3631be956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "√Ånh x·∫° nh√£n: {'Agree': 0, 'Decline': 1, 'Fallback': 2, 'Silence': 3, 'Uncertain': 4}\n",
            "Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 27\n",
            "})\n",
            "Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 10\n",
            "})\n",
            "First row in sample_dataset:\n",
            "{'label': 0, 'input_ids': 'Yes, I want to show you the picture.'}\n"
          ]
        }
      ],
      "source": [
        "# T·ª± ƒë·ªông ph√°t hi·ªán nh√£n v√† t·∫°o √°nh x·∫° nh√£n\n",
        "def create_label_mapping(dataset_list):\n",
        "    \"\"\"\n",
        "    T·ª± ƒë·ªông ph√°t hi·ªán t·∫•t c·∫£ c√°c nh√£n t·ª´ danh s√°ch dataset v√† √°nh x·∫° ch√∫ng th√†nh s·ªë nguy√™n.\n",
        "    \"\"\"\n",
        "    all_labels = set()\n",
        "    for dataset in dataset_list:\n",
        "        all_labels.update(dataset[\"label\"])  # T·∫≠p h·ª£p t·∫•t c·∫£ c√°c nh√£n t·ª´ dataset\n",
        "\n",
        "    label_to_int = {label: idx for idx, label in enumerate(sorted(all_labels))}\n",
        "    print(f\"√Ånh x·∫° nh√£n: {label_to_int}\")\n",
        "    return label_to_int\n",
        "\n",
        "# H√†m chuy·ªÉn ƒë·ªïi nh√£n\n",
        "def preprocess_labels(example, label_to_int):\n",
        "    example[\"label\"] = label_to_int.get(example[\"label\"], -1)  # G√°n -1 cho nh√£n kh√¥ng h·ª£p l·ªá\n",
        "    return example\n",
        "\n",
        "# T·∫°o √°nh x·∫° nh√£n\n",
        "label_mapping = create_label_mapping([dataset])\n",
        "\n",
        "# √Åp d·ª•ng chuy·ªÉn ƒë·ªïi nh√£n\n",
        "dataset = dataset.map(lambda example: preprocess_labels(example, label_mapping))\n",
        "\n",
        "# Ki·ªÉm tra k·∫øt qu·∫£\n",
        "print(dataset)\n",
        "\n",
        "# Truy c·∫≠p m·∫´u c·ª• th·ªÉ\n",
        "sample_dataset = dataset.select(range(10))  # L·∫•y 10 m·∫´u ƒë·∫ßu ti√™n\n",
        "print(sample_dataset)\n",
        "\n",
        "# In th·ª≠ 1 h√†ng trong sample_dataset\n",
        "print(\"First row in sample_dataset:\")\n",
        "print(sample_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hy_jM2gOk99M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "688d6d21-a578-4af1-b147-85cc6c74d75b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chia dataset: 18 m·∫´u train, 9 m·∫´u test\n",
            "Train dataset: Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 18\n",
            "})\n",
            "Test dataset: Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 9\n",
            "})\n",
            "Sample train dataset: Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 8\n",
            "})\n",
            "Sample test dataset: Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 9\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "def split_dataset(dataset, test_size=0.2, seed=42):\n",
        "    \"\"\"\n",
        "    Chia dataset th√†nh t·∫≠p train v√† test.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset): T·∫≠p d·ªØ li·ªáu ƒë·∫ßy ƒë·ªß.\n",
        "        test_size (float): T·ª∑ l·ªá d·ªØ li·ªáu test (0.0 - 1.0).\n",
        "        seed (int): Seed ƒë·ªÉ chia d·ªØ li·ªáu ng·∫´u nhi√™n.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (train_dataset, test_dataset) - T·∫≠p train v√† test.\n",
        "    \"\"\"\n",
        "    if not (0.0 < test_size < 1.0):\n",
        "        raise ValueError(\"test_size ph·∫£i n·∫±m trong kho·∫£ng (0.0, 1.0)\")\n",
        "    if len(dataset) < 2:\n",
        "        raise ValueError(\"Dataset ph·∫£i c√≥ √≠t nh·∫•t 2 m·∫´u ƒë·ªÉ chia.\")\n",
        "\n",
        "    train_test_split = dataset.train_test_split(test_size=test_size, seed=seed)\n",
        "    print(f\"Chia dataset: {len(train_test_split['train'])} m·∫´u train, {len(train_test_split['test'])} m·∫´u test\")\n",
        "    return train_test_split[\"train\"], train_test_split[\"test\"]\n",
        "\n",
        "# Chia dataset\n",
        "train_dataset, test_dataset = split_dataset(dataset, test_size=0.3)\n",
        "\n",
        "# Ki·ªÉm tra d·ªØ li·ªáu\n",
        "print(\"Train dataset:\", train_dataset)\n",
        "print(\"Test dataset:\", test_dataset)\n",
        "\n",
        "# Truy c·∫≠p m·∫´u c·ª• th·ªÉ\n",
        "sample_train_dataset = train_dataset.select(range(8))  # L·∫•y 10 m·∫´u ƒë·∫ßu ti√™n t·ª´ train\n",
        "sample_test_dataset = test_dataset.select(range(9))    # L·∫•y 10 m·∫´u ƒë·∫ßu ti√™n t·ª´ test\n",
        "\n",
        "print(\"Sample train dataset:\", sample_train_dataset)\n",
        "print(\"Sample test dataset:\", sample_test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCzQIjl_ptbw"
      },
      "source": [
        "# 2. Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bknqLH2piJMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94420898-20eb-47ac-fdfc-e42212ca0dc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# B∆∞·ªõc 2: Chu·∫©n b·ªã tokenizer v√† token h√≥a d·ªØ li·ªáu\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir = \"huggingface\")\n",
        "model = BERTIntentClassification(\n",
        "    model_name=model_name,\n",
        "    num_classes=5\n",
        ")\n",
        "model.freeze_bert() # Froze Layer BERT\n",
        "max_seq_length = 512\n",
        "\n",
        "\n",
        "def collate_fn(features):\n",
        "    inputs = []\n",
        "    labels = []\n",
        "    for element in features:\n",
        "        inputs.append(element.get(\"input_ids\"))\n",
        "        labels.append(element.get(\"label\"))\n",
        "\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    token_inputs = tokenizer(\n",
        "        inputs,\n",
        "        add_special_tokens=True,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=max_seq_length,\n",
        "        return_overflowing_tokens=False,\n",
        "        return_length=False,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    token_inputs.update({\n",
        "        \"labels\": labels,\n",
        "    })\n",
        "    return token_inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZldUk54pj1N"
      },
      "source": [
        "# 3. Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptK7Cy22p2GK"
      },
      "source": [
        "## 3.1 Log Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xZkR3vuFp5uN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1242a1a4-416c-4d2b-f292-9f6d2c4fed68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qux2ABzMp7Ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "622c8ba1-69b0-454e-b0c5-48c0b114beb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZZRspV7jp8OT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b08da5-aae7-4a83-f391-5b49b664a472"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c8767\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env\n",
        "load_dotenv()\n",
        "\n",
        "# L·∫•y key t·ª´ bi·∫øn m√¥i tr∆∞·ªùng\n",
        "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
        "print(wandb_api_key[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "215vQ7cOp9oo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "921cb2b6-86fd-4323-8fdc-f772ba36452b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdoanngoccuong\u001b[0m (\u001b[33mdoanngoccuong_nh\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import wandb\n",
        "import os\n",
        "\n",
        "# L·∫•y API key t·ª´ bi·∫øn m√¥i tr∆∞·ªùng v√† ƒëƒÉng nh·∫≠p\n",
        "wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJG_rUYTqwLJ"
      },
      "source": [
        "C√°ch thi·∫øt l·∫≠p th√¥ng qua TrainingArguments\n",
        "Khi s·ª≠ d·ª•ng Trainer, b·∫°n c√≥ th·ªÉ ƒë·∫∑t t√™n d·ª± √°n tr·ª±c ti·∫øp trong TrainingArguments b·∫±ng c√°ch s·ª≠ d·ª•ng tham s·ªë report_to v√† run_name. Tuy nhi√™n, ƒë·ªÉ ƒë·∫∑t project, b·∫°n c·∫ßn kh·ªüi t·∫°o m·ªôt phi√™n wandb tr∆∞·ªõc ho·∫∑c truy·ªÅn c·∫•u h√¨nh n√†y th√¥ng qua wandb.init().\n",
        "\n",
        "ƒêi·ªÅu ch·ªânh TrainingArguments:\n",
        "```python\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_\",          # Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£\n",
        "    eval_strategy=\"epoch\",           # ƒê√°nh gi√° sau m·ªói epoch\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=128,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",            # Th∆∞ m·ª•c l∆∞u log\n",
        "    logging_strategy=\"steps\",        # Log theo steps\n",
        "    logging_steps=10,                # Log sau m·ªói 10 b∆∞·ªõc\n",
        "    save_strategy=\"epoch\",           # L∆∞u checkpoint sau m·ªói epoch\n",
        "    save_total_limit=3,              # L∆∞u t·ªëi ƒëa 3 checkpoint\n",
        "    report_to=\"wandb\",               # B√°o c√°o log t·ªõi wandb\n",
        "    run_name=\"bert_run_1\"            # T√™n phi√™n ch·∫°y tr√™n wandb\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAMgPe3NqAhz"
      },
      "source": [
        "## 3.2 Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3hWcz751mEx"
      },
      "source": [
        "### Ver 1.2.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gs-Z20WNRnE"
      },
      "source": [
        "D∆∞·ªõi ƒë√¢y l√† b·∫£ng t√≥m t·∫Øt chi ti·∫øt c√°ch l∆∞u m√¥ h√¨nh d·ª±a tr√™n chi·∫øn l∆∞·ª£c ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t:\n",
        "\n",
        "| **Lo·∫°i Model**    | **ƒêi·ªÅu Ki·ªán L∆∞u**                                                                 | **Th∆∞ M·ª•c L∆∞u Tr√™n Local**       | **S·ªë L∆∞·ª£ng L∆∞u Tr√™n Local**        | **Th√¥ng Tin Th√™m**                              | **ƒê·ªìng B·ªô L√™n WandB**                  |\n",
        "|--------------------|-----------------------------------------------------------------------------------|-----------------------------------|------------------------------------|-----------------------------------------------|-----------------------------------------|\n",
        "| **Best Model**     | Khi `eval_loss` gi·∫£m                                                             | `output_dir/best_model`           | Ch·ªâ l∆∞u m·ªôt b·∫£n duy nh·∫•t           | L∆∞u th√¥ng tin `epoch` v√† `eval_loss`.          | C√≥: Artifact `best_model`. Th√™m `epoch` v√† `loss` v√†o `metadata`. |\n",
        "| **Final Checkpoint** | Sau m·ªói epoch (checkpoint cu·ªëi c·ªßa epoch)                                        | `output_dir/checkpoint-epoch-<n>` | T·ªëi ƒëa 3 checkpoint g·∫ßn nh·∫•t       | Kh√¥ng c√≥ th√¥ng tin ƒë·∫∑c bi·ªát.                   | Kh√¥ng ƒë·ªìng b·ªô (tr√°nh tr√πng l·∫∑p d·ªØ li·ªáu l·ªõn). |\n",
        "| **Custom Checkpoint** (t√πy ch·ªçn) | Sau m·ªôt s·ªë b∆∞·ªõc c·ªë ƒë·ªãnh ho·∫∑c m·ªëc quan tr·ªçng (n·∫øu c·∫ßn thi·∫øt, v√≠ d·ª•: m·ªói 5 epoch) | T√πy ch·ªânh, v√≠ d·ª•: `output_dir/checkpoint-step-<n>` | Theo √Ω mu·ªën, ho·∫∑c kh√¥ng gi·ªõi h·∫°n | Th√™m c√°c m·ªëc quan tr·ªçng ƒë·ªÉ ph√¢n t√≠ch sau n√†y. | T√πy ch·ªçn (kh√¥ng b·∫Øt bu·ªôc).              |\n",
        "\n",
        "---\n",
        "\n",
        "### **Chi ti·∫øt v·ªÅ b·∫£ng**\n",
        "1. **Best Model**:\n",
        "   - ƒêi·ªÅu ki·ªán: `eval_loss` gi·∫£m.\n",
        "   - Ch·ªâ l∆∞u m·ªôt phi√™n b·∫£n t·ªët nh·∫•t.\n",
        "   - L∆∞u th√¥ng tin epoch v√† loss ƒë·ªÉ d·ªÖ d√†ng tham kh·∫£o ho·∫∑c t·∫£i xu·ªëng sau n√†y.\n",
        "\n",
        "2. **Final Checkpoint**:\n",
        "   - ƒê∆∞·ª£c l∆∞u sau m·ªói epoch.\n",
        "   - Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng checkpoint l∆∞u tr√™n local ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ (v√≠ d·ª•: t·ªëi ƒëa 3 checkpoint).\n",
        "   - Kh√¥ng l∆∞u th√¥ng tin th√™m v√†o checkpoint.\n",
        "\n",
        "3. **Custom Checkpoint** (t√πy ch·ªçn):\n",
        "   - C√≥ th·ªÉ s·ª≠ d·ª•ng n·∫øu b·∫°n mu·ªën l∆∞u checkpoint t·∫°i c√°c m·ªëc th·ªùi gian c·ª• th·ªÉ, ch·∫≥ng h·∫°n nh∆∞ m·ªói 5 epoch ho·∫∑c sau m·ªôt s·ªë b∆∞·ªõc hu·∫•n luy·ªán (steps).\n",
        "   - Th√≠ch h·ª£p khi b·∫°n c·∫ßn ki·ªÉm tra ti·∫øn ƒë·ªô hu·∫•n luy·ªán chi ti·∫øt h∆°n ho·∫∑c mu·ªën l∆∞u backup.\n",
        "\n",
        "---\n",
        "\n",
        "### **T√≥m t·∫Øt logic**\n",
        "- **Best Model**:\n",
        "  - L∆∞u v√†o th∆∞ m·ª•c c·ªë ƒë·ªãnh (`best_model`).\n",
        "  - Ghi ƒë√® khi c√≥ `eval_loss` m·ªõi t·ªët h∆°n.\n",
        "  - ƒê·ªìng b·ªô l√™n WandB.\n",
        "\n",
        "- **Final Checkpoint**:\n",
        "  - L∆∞u sau m·ªói epoch.\n",
        "  - X√≥a checkpoint c≈© nh·∫•t n·∫øu v∆∞·ª£t gi·ªõi h·∫°n `save_total_limit`.\n",
        "  - Kh√¥ng ƒë·ªìng b·ªô l√™n WandB (tr√°nh l√£ng ph√≠ kh√¥ng gian l∆∞u tr·ªØ).\n",
        "\n",
        "- **Custom Checkpoint**:\n",
        "  - T√πy ch·ªçn n·∫øu b·∫°n c·∫ßn l∆∞u th√™m ƒë·ªÉ ph·ª•c v·ª• c√°c m·ª•c ƒë√≠ch c·ª• th·ªÉ.\n",
        "\n",
        "N·∫øu b·∫°n c·∫ßn th√™m b·∫•t k·ª≥ chi ti·∫øt n√†o kh√°c, h√£y cho m√¨nh bi·∫øt nh√©! üòä"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCbkX1KHeT0j"
      },
      "source": [
        "### **B·∫£ng T√≥m T·∫Øt: L∆∞u Best Model v√† Last Model**\n",
        "\n",
        "| **Lo·∫°i Model**    | **Khi N√†o C·∫ßn L∆∞u**                                                                                         | **∆Øu ƒêi·ªÉm**                                                                                       | **H·∫°n Ch·∫ø**                                                                                      |\n",
        "|--------------------|------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|\n",
        "| **Best Model**     | - Khi mu·ªën tri·ªÉn khai m√¥ h√¨nh t·ªët nh·∫•t v·ªõi `eval_loss` th·∫•p nh·∫•t ho·∫∑c `accuracy` cao nh·∫•t.                   | - ƒê·∫£m b·∫£o l∆∞u l·∫°i m√¥ h√¨nh c√≥ hi·ªáu su·∫•t t·ªët nh·∫•t tr√™n t·∫≠p validation.<br>- Ph√π h·ª£p ƒë·ªÉ tri·ªÉn khai.   | - Kh√¥ng l∆∞u tr·∫°ng th√°i ƒë·∫ßy ƒë·ªß (optimizer, scheduler).<br>- Kh√¥ng ti·∫øp t·ª•c hu·∫•n luy·ªán t·ª´ tr·∫°ng th√°i n√†y. |\n",
        "| **Last Model**     | - Khi c·∫ßn ti·∫øp t·ª•c hu·∫•n luy·ªán (fine-tuning) ho·∫∑c kh√¥i ph·ª•c tr·∫°ng th√°i sau khi hu·∫•n luy·ªán k·∫øt th√∫c.         | - L∆∞u ƒë·∫ßy ƒë·ªß tr·∫°ng th√°i (weights, optimizer, scheduler).<br>- Ph√π h·ª£p ƒë·ªÉ ti·∫øp t·ª•c hu·∫•n luy·ªán.    | - C√≥ th·ªÉ kh√¥ng ph·∫£i l√† m√¥ h√¨nh t·ªët nh·∫•t (do overfitting ho·∫∑c underfitting).                     |\n",
        "| **Ch·ªâ L∆∞u Best**   | - Khi ch·ªâ quan t√¢m ƒë·∫øn tri·ªÉn khai m√¥ h√¨nh t·ªët nh·∫•t, kh√¥ng c·∫ßn ti·∫øp t·ª•c hu·∫•n luy·ªán sau n√†y.                  | - Ti·∫øt ki·ªám t√†i nguy√™n l∆∞u tr·ªØ.<br>- T·∫≠p trung v√†o m√¥ h√¨nh t·ªëi ∆∞u cho tri·ªÉn khai.                | - Kh√¥ng th·ªÉ ti·∫øp t·ª•c hu·∫•n luy·ªán n·∫øu c·∫ßn.                                                         |\n",
        "| **Ch·ªâ L∆∞u Last**   | - Khi mu·ªën ƒë·∫£m b·∫£o kh·∫£ nƒÉng kh√¥i ph·ª•c tr·∫°ng th√°i ƒë·ªÉ ti·∫øp t·ª•c hu·∫•n luy·ªán.                                    | - Kh√¥i ph·ª•c ho√†n to√†n qu√° tr√¨nh hu·∫•n luy·ªán.<br>- Ph√π h·ª£p cho fine-tuning ho·∫∑c th·ª≠ nghi·ªám sau n√†y. | - Kh√¥ng ƒë·∫£m b·∫£o ƒë√¢y l√† m√¥ h√¨nh t·ªët nh·∫•t ƒë·ªÉ tri·ªÉn khai.                                           |\n",
        "| **L∆∞u C·∫£ Hai**     | - Khi c·∫ßn c·∫£ tri·ªÉn khai m√¥ h√¨nh t·ªët nh·∫•t v√† ti·∫øp t·ª•c hu·∫•n luy·ªán sau n√†y.                                    | - K·∫øt h·ª£p ∆∞u ƒëi·ªÉm c·ªßa c·∫£ Best Model v√† Last Model.<br>- Linh ho·∫°t trong s·ª≠ d·ª•ng.                 | - T·ªën th√™m t√†i nguy√™n l∆∞u tr·ªØ v√† th·ªùi gian.                                                     |\n",
        "\n",
        "---\n",
        "\n",
        "### **Chi·∫øn L∆∞·ª£c T·ªëi ∆Øu**\n",
        "| **Lo·∫°i L∆∞u** | **T·∫ßn Su·∫•t**                          | **Chi·∫øn L∆∞·ª£c**                                                                                             |\n",
        "|--------------|---------------------------------------|-----------------------------------------------------------------------------------------------------------|\n",
        "| **Best Model** | Khi `eval_loss` gi·∫£m                 | L∆∞u m·ªói l·∫ßn `eval_loss` gi·∫£m ƒë·ªÉ ƒë·∫£m b·∫£o m√¥ h√¨nh t·ªët nh·∫•t lu√¥n ƒë∆∞·ª£c l∆∞u.                                    |\n",
        "| **Last Model** | Sau khi hu·∫•n luy·ªán k·∫øt th√∫c          | L∆∞u tr·∫°ng th√°i cu·ªëi c√πng c·ªßa qu√° tr√¨nh hu·∫•n luy·ªán (weights + optimizer + scheduler).                      |\n",
        "| **K·∫øt h·ª£p**   | Best Model: M·ªói khi `eval_loss` gi·∫£m<br>Last Model: Sau khi k·∫øt th√∫c | L∆∞u c·∫£ Best Model ƒë·ªÉ tri·ªÉn khai v√† Last Model ƒë·ªÉ ti·∫øp t·ª•c hu·∫•n luy·ªán khi c·∫ßn thi·∫øt.                      |\n",
        "\n",
        "---\n",
        "\n",
        "### **L·ª±a Ch·ªçn Ph√π H·ª£p**\n",
        "- **D·ª± √°n tri·ªÉn khai m√¥ h√¨nh nhanh**: L∆∞u **Best Model**.\n",
        "- **D·ª± √°n nghi√™n c·ª©u ho·∫∑c fine-tuning ti·∫øp**: L∆∞u **Last Model**.\n",
        "- **D·ª± √°n quy m√¥ l·ªõn, c·∫ßn c·∫£ tri·ªÉn khai v√† m·ªü r·ªông**: L∆∞u **c·∫£ hai**.\n",
        "\n",
        "H√£y ch·ªçn chi·∫øn l∆∞·ª£c l∆∞u ph√π h·ª£p v·ªõi m·ª•c ti√™u d·ª± √°n c·ªßa b·∫°n! üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLIKjrdaesyG"
      },
      "source": [
        "Thui, ko l∆∞u local n·ªØa, l∆∞u t·∫•t tr√™n wandb ƒëi.\n",
        "- V·ªõi best model: l∆∞u l√™n wandb khi loss gi·∫£m v√† ƒë√£ sau 10 epochs  \n",
        "(L∆∞u Best Model ngay khi eval_loss gi·∫£m ·ªü local, sau 10 epochs th√¨ ƒë·ªìng b·ªô c√°i best l√™n wandb, sau ƒë√≥ xo√° c√°c file best ·ªü local).\n",
        "Ch·ªâ ƒë·ªìng b·ªô l√™n WandB m·ªói 10 epochs.)\n",
        "- V·ªõi last model: l∆∞u l√™n wandb sau m·ªói 10 epochs. (l∆∞u local tr∆∞·ªõc -> ƒë·ªìng b·ªô l√™n wandb s·∫Ω xo√° file local)\n",
        "+, Trong qu√° tr√¨nh l∆∞u th√¨ vi·ªác training v·∫´n di·ªÖn ra Parallel\n",
        "\n",
        "ƒë·ªÅu l∆∞u ƒë·∫ßy ƒë·ªß to√†n b·ªô tham s·ªë ƒë·ªÉ c√≥ th·ªÉ train th√™m t·ª´ c·∫£ ·ªü best model v√† last model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bHYLAOxP3Lea"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "I5n7vK-a9_gM"
      },
      "outputs": [],
      "source": [
        "# class TrainerCustom(Trainer):\n",
        "\n",
        "#     def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "#         \"\"\"\n",
        "#         How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
        "\n",
        "#         Subclass and override for custom behavior.\n",
        "#         \"\"\"\n",
        "#         if \"labels\" in inputs:\n",
        "#             labels = inputs.pop(\"labels\")\n",
        "#         else:\n",
        "#             labels = None\n",
        "\n",
        "#         # S·ª≠ d·ª•ng nn.CrossEntropyLoss() thay v√¨ nn.CrossEntropy\n",
        "#         cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "#         # Ch·∫°y m√¥ h√¨nh v√† nh·∫≠n ƒë·∫ßu ra (logits)\n",
        "#         outputs = model(**inputs)\n",
        "\n",
        "#         # ƒê·∫£m b·∫£o l·∫•y logits t·ª´ outputs (m√¥ h√¨nh tr·∫£ v·ªÅ tuple, l·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n l√† logits)\n",
        "#         logits = outputs\n",
        "\n",
        "#         if labels is None:\n",
        "#             print(\"Labels are None during compute_loss.\")\n",
        "#         if logits is None:\n",
        "#             print(\"Logits are None during compute_loss.\")\n",
        "\n",
        "#         # T√≠nh to√°n loss\n",
        "#         loss = cross_entropy_loss(logits, labels)\n",
        "\n",
        "#         # Tr·∫£ v·ªÅ loss v√† outputs n·∫øu c·∫ßn\n",
        "#         return (loss, outputs) if return_outputs else loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "iYRbPjlfDSTs"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import wandb\n",
        "\n",
        "# # Kh·ªüi t·∫°o wandb\n",
        "# wandb.init(\n",
        "#     project=\"bert-intent-classification\",  # T√™n d·ª± √°n\n",
        "#     name=\"bert_run_3\"                     # T√™n phi√™n ch·∫°y\n",
        "# )\n",
        "\n",
        "\n",
        "# # B∆∞·ªõc 6: C√†i ƒë·∫∑t tham s·ªë hu·∫•n luy·ªán\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./result__s\",          # Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£\n",
        "#     eval_strategy=\"epoch\",    # ƒê√°nh gi√° sau m·ªói epoch\n",
        "#     learning_rate=2e-4,\n",
        "#     per_device_train_batch_size=128,\n",
        "#     per_device_eval_batch_size=128,\n",
        "#     num_train_epochs=50,\n",
        "#     weight_decay=0.01,\n",
        "#     logging_dir=\"./logs\",\n",
        "#     logging_strategy=\"steps\",\n",
        "#     logging_steps=1,  # Ghi logs m·ªói 500 b∆∞·ªõc hu·∫•n luy·ªán\n",
        "#     save_strategy=\"no\",          # L∆∞u tr·ªçng s·ªë sau m·ªói epoch\n",
        "#     save_total_limit=3,\n",
        "#     label_names = [\"labels\"],\n",
        "#     report_to=\"wandb\",\n",
        "#     run_name=\"bert_run_3\"\n",
        "# )\n",
        "\n",
        "\n",
        "# batch = collate_fn([sample_test_dataset[0]]) # T·∫°o m·ªôt batch t·ª´ m·ªôt m·∫´u ƒë∆°n l·∫ª (sample_test_dataset[0]) ƒë·ªÉ ki·ªÉm tra xem h√†m collate_fn c√≥ ho·∫°t ƒë·ªông ƒë√∫ng kh√¥ng.\n",
        "# print(batch)\n",
        "\n",
        "# # metrics = trainer.evaluate()\n",
        "# # M·ª•c ƒë√≠ch: Ch·∫°y giai ƒëo·∫°n evaluation (ƒë√°nh gi√°) tr√™n eval_dataset (sample_test_dataset) v√† t√≠nh to√°n c√°c metrics nh∆∞:\n",
        "# trainer = TrainerCustom(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=sample_train_dataset,\n",
        "#     eval_dataset=sample_test_dataset,\n",
        "#     tokenizer=tokenizer,\n",
        "#     data_collator=collate_fn,\n",
        "# )\n",
        "\n",
        "# metrics = trainer.evaluate()\n",
        "# print(metrics)  # Ki·ªÉm tra xem c√≥ \"eval_loss\" hay kh√¥ng\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # B∆∞·ªõc 7: T·∫°o Trainer\n",
        "# trainer = TrainerCustom(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=sample_train_dataset,\n",
        "#     eval_dataset=sample_test_dataset,\n",
        "#     tokenizer=tokenizer,\n",
        "#     data_collator = collate_fn,\n",
        "# )\n",
        "\n",
        "# # B∆∞·ªõc 8: Hu·∫•n luy·ªán\n",
        "# trainer.train()\n",
        "\n",
        "# # K·∫øt th√∫c phi√™n wandb\n",
        "# wandb.finish()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vtlo08BaWQhe"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import wandb\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "class TrainerCustom(Trainer):\n",
        "    def __init__(self, *args, save_every_n_epochs=10, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"Trainer is running on GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
        "        else:\n",
        "            print(\"Trainer is running on CPU.\")\n",
        "\n",
        "        self.best_eval_loss = float(\"inf\")  # Gi√° tr·ªã loss t·ªët nh·∫•t ban ƒë·∫ßu\n",
        "        self.save_every_n_epochs = save_every_n_epochs  # T·∫ßn su·∫•t l∆∞u l√™n WandB\n",
        "        self.best_model_info = {\"epoch\": None, \"loss\": None}\n",
        "        self.last_saved_epoch = 0  # Epoch cu·ªëi c√πng ƒë√£ l∆∞u Best Model v√† Last Model\n",
        "        self.executor = ThreadPoolExecutor(max_workers=3)  # Cho ph√©p t·ªëi ƒëa 2 lu·ªìng song song\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        \"\"\"\n",
        "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
        "\n",
        "        Subclass and override for custom behavior.\n",
        "        \"\"\"\n",
        "\n",
        "        # Ki·ªÉm tra thi·∫øt b·ªã c·ªßa m√¥ h√¨nh v√† d·ªØ li·ªáu\n",
        "        print(\"Model device:\", next(model.parameters()).device)\n",
        "        print(\"Input device:\", inputs[\"input_ids\"].device)\n",
        "        if \"labels\" in inputs:\n",
        "            labels = inputs.pop(\"labels\")\n",
        "        else:\n",
        "            labels = None\n",
        "\n",
        "        # S·ª≠ d·ª•ng nn.CrossEntropyLoss() thay v√¨ nn.CrossEntropy\n",
        "        cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Ch·∫°y m√¥ h√¨nh v√† nh·∫≠n ƒë·∫ßu ra (logits)\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # ƒê·∫£m b·∫£o l·∫•y logits t·ª´ outputs (m√¥ h√¨nh tr·∫£ v·ªÅ tuple, l·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n l√† logits)\n",
        "        logits = outputs\n",
        "\n",
        "        if labels is None:\n",
        "            print(\"Labels are None during compute_loss.\")\n",
        "        if logits is None:\n",
        "            print(\"Logits are None during compute_loss.\")\n",
        "\n",
        "        # T√≠nh to√°n loss\n",
        "        loss = cross_entropy_loss(logits, labels)\n",
        "\n",
        "        # Tr·∫£ v·ªÅ loss v√† outputs n·∫øu c·∫ßn\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    def async_save_model(self, model_dir, artifact_name, metadata=None):\n",
        "        \"\"\"\n",
        "        L∆∞u m√¥ h√¨nh v√†o local v√† ƒë·ªìng b·ªô l√™n WandB trong lu·ªìng song song.\n",
        "        \"\"\"\n",
        "        def save():\n",
        "            start_time = time.time()\n",
        "            try:\n",
        "                # X√≥a t·∫•t c·∫£ c√°c th∆∞ m·ª•c tmp_best_model_ tr∆∞·ªõc ƒë√≥\n",
        "                for folder in os.listdir(\".\"):\n",
        "                    if folder.startswith(\"tmp_best_model_epoch_\") and folder != model_dir:\n",
        "                        shutil.rmtree(folder, ignore_errors=True)\n",
        "                        print(f\"Removed old temporary directory: {folder}\")\n",
        "\n",
        "                # L∆∞u m√¥ h√¨nh v√†o th∆∞ m·ª•c t·∫°m\n",
        "                self.save_model(model_dir)\n",
        "\n",
        "                # ƒê·ªìng b·ªô l√™n WandB\n",
        "                artifact = wandb.Artifact(artifact_name, type=\"model\")\n",
        "                artifact.add_dir(model_dir)\n",
        "                if metadata:\n",
        "                    artifact.metadata = metadata\n",
        "                wandb.log_artifact(artifact)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during saving or syncing model {artifact_name}: {e}\")\n",
        "            finally:\n",
        "                # X√≥a th∆∞ m·ª•c t·∫°m hi·ªán t·∫°i sau khi ƒë·ªìng b·ªô\n",
        "                try:\n",
        "                    shutil.rmtree(model_dir, ignore_errors=True)\n",
        "                    print(f\"Successfully removed temporary directory: {model_dir}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error removing temporary directory {model_dir}: {e}\")\n",
        "\n",
        "            elapsed_time = time.time() - start_time\n",
        "            print(f\"Model saved and uploaded to WandB: {artifact_name} in {elapsed_time:.2f} seconds\")\n",
        "\n",
        "        self.executor.submit(save)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix: str = \"eval\"):\n",
        "        metrics = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
        "        eval_loss = metrics.get(\"eval_loss\")\n",
        "\n",
        "        # C·∫≠p nh·∫≠t Best Model n·∫øu eval_loss gi·∫£m\n",
        "        # L∆∞u Best Model ngay khi eval_loss gi·∫£m (local).\n",
        "        # Ch·ªâ ƒë·ªìng b·ªô l√™n WandB m·ªói 10 epochs.\n",
        "\n",
        "        if eval_loss is not None and eval_loss < self.best_eval_loss:\n",
        "            print(f\"New best eval_loss: {eval_loss}\")\n",
        "            self.best_eval_loss = eval_loss\n",
        "            self.best_model_info = {\"epoch\": self.state.epoch, \"loss\": eval_loss}\n",
        "\n",
        "            # Log th√¥ng tin Best Model l√™n WandB\n",
        "            wandb.log({\n",
        "                \"best_eval_loss\": self.best_eval_loss,\n",
        "                \"best_model_epoch\": self.best_model_info.get(\"epoch\", -1)\n",
        "            })\n",
        "\n",
        "            # L∆∞u Best Model v√†o th∆∞ m·ª•c t·∫°m (local)\n",
        "            best_model_dir = f\"./tmp_best_model_epoch_{int(self.state.epoch)}\"\n",
        "            self.save_model(best_model_dir)\n",
        "\n",
        "            # ƒê·ªìng b·ªô l√™n WandB m·ªói 10 epochs\n",
        "            if int(self.state.epoch) % self.save_every_n_epochs == 0:\n",
        "                artifact_name = f\"best_model_epoch_{int(self.state.epoch)}\"\n",
        "                self.async_save_model(best_model_dir, artifact_name, self.best_model_info)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def save_last_model(self):\n",
        "        \"\"\"\n",
        "        L∆∞u Last Model l√™n WandB sau m·ªói N epochs.\n",
        "        \"\"\"\n",
        "        if int(self.state.epoch) % self.save_every_n_epochs == 0 and int(self.state.epoch) != self.last_saved_epoch:\n",
        "            print(f\"Saving Last Model at epoch {self.state.epoch} to WandB...\")\n",
        "            last_model_dir = f\"./tmp_last_model_epoch_{int(self.state.epoch)}\"\n",
        "            artifact_name = f\"last_model_epoch_{int(self.state.epoch)}\"\n",
        "            self.async_save_model(last_model_dir, artifact_name)\n",
        "\n",
        "            # Log th√¥ng tin Last Model l√™n WandB\n",
        "            wandb.log({\n",
        "                \"last_model_epoch\": self.state.epoch\n",
        "            })\n",
        "\n",
        "            # C·∫≠p nh·∫≠t epoch cu·ªëi c√πng ƒë√£ l∆∞u\n",
        "            self.last_saved_epoch = int(self.state.epoch)\n",
        "\n",
        "    def train(self, *args, **kwargs):\n",
        "        result = super().train(*args, **kwargs)\n",
        "\n",
        "        # Sau m·ªói epoch, l∆∞u Last Model l√™n WandB\n",
        "        self.save_last_model()\n",
        "        # Ch·ªù t·∫•t c·∫£ c√°c lu·ªìng l∆∞u ho√†n th√†nh tr∆∞·ªõc khi k·∫øt th√∫c\n",
        "        self.executor.shutdown(wait=True)\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "# B∆∞·ªõc 6: C√†i ƒë·∫∑t tham s·ªë hu·∫•n luy·ªán\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./result__s\",          # Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£\n",
        "    eval_strategy=\"epoch\",    # ƒê√°nh gi√° sau m·ªói epoch\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=128,\n",
        "    num_train_epochs=50,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=1,  # Ghi logs m·ªói 500 b∆∞·ªõc hu·∫•n luy·ªán\n",
        "    save_strategy=\"no\",          # L∆∞u tr·ªçng s·ªë sau m·ªói epoch\n",
        "    save_total_limit=3,\n",
        "    label_names = [\"labels\"],\n",
        "    report_to=\"wandb\",\n",
        "    run_name=\"bert_run_3\"\n",
        ")\n",
        "\n",
        "\n",
        "import wandb\n",
        "\n",
        "# Kh·ªüi t·∫°o wandb\n",
        "wandb.init(\n",
        "    project=\"bert-intent-classification\",  # T√™n d·ª± √°n\n",
        "    name=\"bert_run_3\",                     # T√™n phi√™n ch·∫°y\n",
        "    config={\"gpu\": torch.cuda.get_device_name(torch.cuda.current_device()) if torch.cuda.is_available() else \"CPU\"}\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Trainer is running on GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
        "else:\n",
        "    print(\"Trainer is running on CPU.\")\n",
        "\n",
        "trainer = TrainerCustom(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=sample_train_dataset,\n",
        "    eval_dataset=sample_test_dataset,\n",
        "    data_collator=collate_fn,\n",
        "    save_every_n_epochs=10  # L∆∞u Best Model v√† Last Model m·ªói 10 epochs\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7qjF1K2EW0Ys",
        "outputId": "71d5827b-d426-4aaf-95d0-125065e8b2aa"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250113_155341-ybo3fi9m</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/doanngoccuong_nh/bert-intent-classification/runs/ybo3fi9m' target=\"_blank\">bert_run_3</a></strong> to <a href='https://wandb.ai/doanngoccuong_nh/bert-intent-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/doanngoccuong_nh/bert-intent-classification' target=\"_blank\">https://wandb.ai/doanngoccuong_nh/bert-intent-classification</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/doanngoccuong_nh/bert-intent-classification/runs/ybo3fi9m' target=\"_blank\">https://wandb.ai/doanngoccuong_nh/bert-intent-classification/runs/ybo3fi9m</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainer is running on GPU: Tesla T4\n",
            "Trainer is running on GPU: Tesla T4\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:07, Epoch 50/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.504333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.508570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.512193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.514808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.518519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.519195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.520812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.521818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.524044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.525590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.527027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.528899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.530765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.532099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.532809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.533924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.537579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.541591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.543557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.545702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.547019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.550053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.552818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.555238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.557191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.559164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.561520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.564117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.567102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.570376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.573700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.575893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.578464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.580347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.581907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.583474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.585416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.586971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.589063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.591239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.593091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.594595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.596463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.598260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.599729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.600888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.601807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.602536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.603000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.603219</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "New best eval_loss: 3.5043325424194336\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Saving Last Model at epoch 50.0 to WandB...\n",
            "Removed old temporary directory: tmp_best_model_epoch_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./tmp_last_model_epoch_50)... Done. 5.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully removed temporary directory: ./tmp_last_model_epoch_50\n",
            "Model saved and uploaded to WandB: last_model_epoch_50 in 13.87 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_eval_loss</td><td>‚ñÅ</td></tr><tr><td>best_model_epoch</td><td>‚ñÅ</td></tr><tr><td>eval/loss</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñà‚ñÜ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñÑ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñá‚ñà‚ñá‚ñÉ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñà‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñÑ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñá‚ñà‚ñà‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÜ‚ñá‚ñÅ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>last_model_epoch</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñà‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_eval_loss</td><td>3.50433</td></tr><tr><td>best_model_epoch</td><td>1</td></tr><tr><td>eval/loss</td><td>3.60322</td></tr><tr><td>eval/runtime</td><td>0.0371</td></tr><tr><td>eval/samples_per_second</td><td>242.832</td></tr><tr><td>eval/steps_per_second</td><td>26.981</td></tr><tr><td>last_model_epoch</td><td>50</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>50</td></tr><tr><td>train/global_step</td><td>50</td></tr><tr><td>train/grad_norm</td><td>0.0</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>0</td></tr><tr><td>train_loss</td><td>0.0</td></tr><tr><td>train_runtime</td><td>7.7718</td></tr><tr><td>train_samples_per_second</td><td>51.468</td></tr><tr><td>train_steps_per_second</td><td>6.434</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">bert_run_3</strong> at: <a href='https://wandb.ai/doanngoccuong_nh/bert-intent-classification/runs/ybo3fi9m' target=\"_blank\">https://wandb.ai/doanngoccuong_nh/bert-intent-classification/runs/ybo3fi9m</a><br> View project at: <a href='https://wandb.ai/doanngoccuong_nh/bert-intent-classification' target=\"_blank\">https://wandb.ai/doanngoccuong_nh/bert-intent-classification</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250113_155341-ybo3fi9m/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# B∆∞·ªõc 9: ƒê√°nh gi√° tr√™n t·∫≠p ki·ªÉm tra\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "rC3AmIe0T9co",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "3649d606-a0b3-4bc2-b680-94d2f7d58c40"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 : < :]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "You must call wandb.init() before wandb.log()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-d56dd7597d91>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# B∆∞·ªõc 9: ƒê√°nh gi√° tr√™n t·∫≠p ki·ªÉm tra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-8962e5ce1ff0>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0meval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"eval_loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4072\u001b[0m         )\n\u001b[1;32m   4073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4074\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4076\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mDebugOption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU_METRICS_DEBUG\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mlog\u001b[0;34m(self, logs, start_time)\u001b[0m\n\u001b[1;32m   3568\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3569\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3570\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3572\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prepare_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mon_log\u001b[0;34m(self, args, state, control, logs)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_log\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_prediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mcall_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             result = getattr(callback, event)(\n\u001b[0m\u001b[1;32m    520\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36mon_log\u001b[0;34m(self, args, state, control, model, logs, **kwargs)\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0mnon_scalar_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msingle_value_scalars\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mnon_scalar_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewrite_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_scalar_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnon_scalar_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train/global_step\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/preinit.py\u001b[0m in \u001b[0;36mpreinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m ) -> Callable:\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"You must call wandb.init() before {name}()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiq5bRFTmzv5"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"What is the weather like today?\"\n",
        "\n",
        "\n",
        "inputs = tokenizer(\n",
        "    sentence,\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "am0boXLrUCXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # ƒê·∫∑t m√¥ h√¨nh ·ªü ch·∫ø ƒë·ªô ƒë√°nh gi√° (kh√¥ng t√≠nh gradient)\n",
        "with torch.no_grad():  # Kh√¥ng c·∫ßn t√≠nh gradient\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predicted_class = torch.argmax(logits, dim=1).item()  # L·∫•y nh√£n d·ª± ƒëo√°n\n",
        "    print(f\"Predicted class: {predicted_class}\")\n"
      ],
      "metadata": {
        "id": "yB4bStFOUDkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTTbVHd551js"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}