{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLWoGrdRXTSg"
      },
      "source": [
        "## 1. T·∫£i D·ªØ Li·ªáu t·ª´ CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbMKcafkKCPk",
        "outputId": "d317166c-64a0-40b4-a2b8-ebddc8abd10a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zIVYywvyiq2L"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModel\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" ## Setup CUDA GPU 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IkfPfDY3iskg"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BERTIntentClassification(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, model_name=\"bert-base-uncased\", num_classes=10, dropout_rate=0.1, cache_dir = \"huggingface\"):\n",
        "        super(BERTIntentClassification, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name, cache_dir = cache_dir)\n",
        "        # Get BERT hidden size\n",
        "        hidden_size = self.bert.config.hidden_size\n",
        "        self.ffnn = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.LayerNorm(hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size, num_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "    def freeze_bert(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "    def get_pooling(self, hidden_state, attention_mask):\n",
        "        \"\"\"\n",
        "        Get mean pooled representation from BERT hidden states\n",
        "        Args:\n",
        "            hidden_state: BERT output containing hidden states\n",
        "        Returns:\n",
        "            pooled_output: Mean pooled representation of the sequence\n",
        "        \"\"\"\n",
        "        # Get last hidden state\n",
        "        last_hidden_state = hidden_state.last_hidden_state  # Shape: [batch_size, seq_len, hidden_size]\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # Expand attention mask to match hidden state dimensions\n",
        "            attention_mask = attention_mask.unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
        "\n",
        "            # Mask out padding tokens\n",
        "            masked_hidden = last_hidden_state * attention_mask\n",
        "\n",
        "            # Calculate mean (sum / number of actual tokens)\n",
        "            sum_hidden = torch.sum(masked_hidden, dim=1)  # [batch_size, hidden_size]\n",
        "            count_tokens = torch.sum(attention_mask, dim=1)  # [batch_size, 1]\n",
        "            pooled_output = sum_hidden / count_tokens\n",
        "        else:\n",
        "            # If no attention mask, simply take mean of all tokens\n",
        "            pooled_output = torch.mean(last_hidden_state, dim=1)\n",
        "\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, **kwargs):\n",
        "        \"\"\"\n",
        "        Forward pass of the model\n",
        "        Args:\n",
        "            input_ids: Input token IDs\n",
        "            attention_mask: Attention mask for padding\n",
        "        Returns:\n",
        "            logits: Raw logits for each class\n",
        "        \"\"\"\n",
        "        # Get BERT hidden states\n",
        "        hidden_state = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        # Get pooled representation\n",
        "        hidden_state_pooling = self.get_pooling(hidden_state=hidden_state, attention_mask=attention_mask)\n",
        "\n",
        "        # Pass through FFNN classifier\n",
        "        logits = self.ffnn(hidden_state_pooling)\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zypDXvoaivAb"
      },
      "outputs": [],
      "source": [
        "class TrainerCustom(Trainer):\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        \"\"\"\n",
        "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
        "\n",
        "        Subclass and override for custom behavior.\n",
        "        \"\"\"\n",
        "        if \"labels\" in inputs:\n",
        "            labels = inputs.pop(\"labels\")\n",
        "        else:\n",
        "            labels = None\n",
        "\n",
        "        # S·ª≠ d·ª•ng nn.CrossEntropyLoss() thay v√¨ nn.CrossEntropy\n",
        "        cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Ch·∫°y m√¥ h√¨nh v√† nh·∫≠n ƒë·∫ßu ra (logits)\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # ƒê·∫£m b·∫£o l·∫•y logits t·ª´ outputs (m√¥ h√¨nh tr·∫£ v·ªÅ tuple, l·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n l√† logits)\n",
        "        logits = outputs\n",
        "\n",
        "        # T√≠nh to√°n loss\n",
        "        loss = cross_entropy_loss(logits, labels)\n",
        "\n",
        "        # Tr·∫£ v·ªÅ loss v√† outputs n·∫øu c·∫ßn\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zIJIYGcppMk"
      },
      "source": [
        "# 1. Load Dataset and with Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DQBY8_d5rwlj"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # B∆∞·ªõc 1: T·∫£i d·ªØ li·ªáu\n",
        "# # S·ª≠ d·ª•ng dataset s·∫µn c√≥ t·ª´ Hugging Face ho·∫∑c t·∫£i t·ª´ file c·ª•c b·ªô\n",
        "# dataset = load_dataset(\"imdb\", cache_dir = \"huggingface\")  # V√≠ d·ª•: D·ªØ li·ªáu IMDB ƒë·ªÉ ph√¢n lo·∫°i sentiment\n",
        "# # Thay th·∫ø tr∆∞·ªùng 'text' th√†nh 'input_ids' trong train_dataset v√† test_dataset\n",
        "# def preprocess_dataset(dataset):\n",
        "#     return dataset.map(lambda example: {\n",
        "#             \"input_ids\": example['text'],\n",
        "#             \"label\": example['label']\n",
        "#         },\n",
        "#         remove_columns=[\"text\"],\n",
        "#         num_proc=4  # S·ª≠ d·ª•ng 4 ti·∫øn tr√¨nh song song ƒë·ªÉ x·ª≠ l√Ω nhanh h∆°n\n",
        "#     )\n",
        "\n",
        "# train_dataset = preprocess_dataset(dataset[\"train\"])\n",
        "# test_dataset = preprocess_dataset(dataset[\"test\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RBMrLoabi9DJ"
      },
      "outputs": [],
      "source": [
        "# print(train_dataset)\n",
        "# # Truy c·∫≠p m·∫´u c·ª• th·ªÉ\n",
        "# train_sample = train_dataset[:10]\n",
        "# test_sample = test_dataset[:2]\n",
        "# print(train_sample)\n",
        "\n",
        "\n",
        "# from datasets import Dataset\n",
        "\n",
        "# train_sample = train_dataset[:10]\n",
        "\n",
        "# # Chuy·ªÉn t·ª´ dict v·ªÅ Dataset\n",
        "# train_sample_dataset = Dataset.from_dict(train_sample)\n",
        "# test_sample_dataset = Dataset.from_dict(test_sample)\n",
        "# print(train_sample_dataset)\n",
        "# print(type(train_sample_dataset))\n",
        "# # Output: <class 'datasets.arrow_dataset.Dataset'>\n",
        "\n",
        "\n",
        "# # In th·ª≠ 1 h√†ng trong test_sample_dataset\n",
        "# print(\"First row in test_sample_dataset:\")\n",
        "# print(test_sample_dataset[0])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "slaqKCOhjh9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f79adf3c-8d16-47b2-f13f-bcbb5a82579f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 27\n",
            "})\n",
            "Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 10\n",
            "})\n",
            "First row in test_sample_dataset:\n",
            "{'label': 'Agree', 'input_ids': 'Yes, I want to show you the picture.'}\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def load_csv_dataset(csv_path, text_column, label_column):\n",
        "    \"\"\"\n",
        "    T·∫£i dataset t·ª´ file CSV v√† ƒë·ªïi t√™n c·ªôt.\n",
        "\n",
        "    Args:\n",
        "        csv_path (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn file .csv.\n",
        "        text_column (str): T√™n c·ªôt ch·ª©a vƒÉn b·∫£n.\n",
        "        label_column (str): T√™n c·ªôt ch·ª©a nh√£n.\n",
        "\n",
        "    Returns:\n",
        "        Dataset: T·∫≠p d·ªØ li·ªáu ƒë√£ t·∫£i t·ª´ file .csv.\n",
        "    \"\"\"\n",
        "    # T·∫£i d·ªØ li·ªáu t·ª´ file .csv\n",
        "    dataset = Dataset.from_csv(csv_path)\n",
        "    # ƒê·ªïi t√™n c·ªôt\n",
        "    dataset = dataset.rename_columns({text_column: \"input_ids\", label_column: \"label\"})\n",
        "    return dataset\n",
        "\n",
        "# S·ª≠ d·ª•ng h√†m\n",
        "csv_path = \"/content/chatbot_intent_data_v1_En.csv\"             # ƒê∆∞·ªùng d·∫´n file CSV\n",
        "text_column = \"input_ids\"       # C·ªôt ch·ª©a vƒÉn b·∫£n\n",
        "label_column = \"label\"        # C·ªôt ch·ª©a nh√£n\n",
        "\n",
        "# T·∫£i dataset\n",
        "dataset = load_csv_dataset(csv_path, text_column, label_column)\n",
        "\n",
        "# Ki·ªÉm tra d·ªØ li·ªáu\n",
        "print(dataset)\n",
        "\n",
        "# Truy c·∫≠p m·∫´u c·ª• th·ªÉ\n",
        "sample_dataset = dataset.select(range(10))  # L·∫•y 10 m·∫´u ƒë·∫ßu ti√™n\n",
        "print(sample_dataset)\n",
        "\n",
        "\n",
        "# In th·ª≠ 1 h√†ng trong test_sample_dataset\n",
        "print(\"First row in test_sample_dataset:\")\n",
        "print(sample_dataset[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1oFci1j5k1UY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fd72d6f-294d-4aa9-a731-11b40b2c6e23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Invalid Samples =====\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "def check_invalid_samples(dataset):\n",
        "    invalid_samples = []\n",
        "    for idx, sample in enumerate(dataset):\n",
        "        if not isinstance(sample[\"input_ids\"], str) or sample[\"input_ids\"].strip() == \"\":\n",
        "            invalid_samples.append((idx, sample))\n",
        "    return invalid_samples\n",
        "\n",
        "# Ki·ªÉm tra d·ªØ li·ªáu kh√¥ng h·ª£p l·ªá\n",
        "invalid_samples = check_invalid_samples(dataset)\n",
        "print(\"\\n===== Invalid Samples =====\")\n",
        "print(invalid_samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "x0-3V7dLlCZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79cd02ff-f8c1-49d9-9c53-01686239d169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "√Ånh x·∫° nh√£n: {'Agree': 0, 'Decline': 1, 'Fallback': 2, 'Silence': 3, 'Uncertain': 4}\n",
            "Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 27\n",
            "})\n",
            "Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 10\n",
            "})\n",
            "First row in sample_dataset:\n",
            "{'label': 0, 'input_ids': 'Yes, I want to show you the picture.'}\n"
          ]
        }
      ],
      "source": [
        "# T·ª± ƒë·ªông ph√°t hi·ªán nh√£n v√† t·∫°o √°nh x·∫° nh√£n\n",
        "def create_label_mapping(dataset_list):\n",
        "    \"\"\"\n",
        "    T·ª± ƒë·ªông ph√°t hi·ªán t·∫•t c·∫£ c√°c nh√£n t·ª´ danh s√°ch dataset v√† √°nh x·∫° ch√∫ng th√†nh s·ªë nguy√™n.\n",
        "    \"\"\"\n",
        "    all_labels = set()\n",
        "    for dataset in dataset_list:\n",
        "        all_labels.update(dataset[\"label\"])  # T·∫≠p h·ª£p t·∫•t c·∫£ c√°c nh√£n t·ª´ dataset\n",
        "\n",
        "    label_to_int = {label: idx for idx, label in enumerate(sorted(all_labels))}\n",
        "    print(f\"√Ånh x·∫° nh√£n: {label_to_int}\")\n",
        "    return label_to_int\n",
        "\n",
        "# H√†m chuy·ªÉn ƒë·ªïi nh√£n\n",
        "def preprocess_labels(example, label_to_int):\n",
        "    example[\"label\"] = label_to_int.get(example[\"label\"], -1)  # G√°n -1 cho nh√£n kh√¥ng h·ª£p l·ªá\n",
        "    return example\n",
        "\n",
        "# T·∫°o √°nh x·∫° nh√£n\n",
        "label_mapping = create_label_mapping([dataset])\n",
        "\n",
        "# √Åp d·ª•ng chuy·ªÉn ƒë·ªïi nh√£n\n",
        "dataset = dataset.map(lambda example: preprocess_labels(example, label_mapping))\n",
        "\n",
        "# Ki·ªÉm tra k·∫øt qu·∫£\n",
        "print(dataset)\n",
        "\n",
        "# Truy c·∫≠p m·∫´u c·ª• th·ªÉ\n",
        "sample_dataset = dataset.select(range(10))  # L·∫•y 10 m·∫´u ƒë·∫ßu ti√™n\n",
        "print(sample_dataset)\n",
        "\n",
        "# In th·ª≠ 1 h√†ng trong sample_dataset\n",
        "print(\"First row in sample_dataset:\")\n",
        "print(sample_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hy_jM2gOk99M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed8a7c70-cf2a-4c68-e78e-62bb817d8a17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chia dataset: 18 m·∫´u train, 9 m·∫´u test\n",
            "Train dataset: Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 18\n",
            "})\n",
            "Test dataset: Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 9\n",
            "})\n",
            "Sample train dataset: Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 8\n",
            "})\n",
            "Sample test dataset: Dataset({\n",
            "    features: ['label', 'input_ids'],\n",
            "    num_rows: 9\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "def split_dataset(dataset, test_size=0.2, seed=42):\n",
        "    \"\"\"\n",
        "    Chia dataset th√†nh t·∫≠p train v√† test.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset): T·∫≠p d·ªØ li·ªáu ƒë·∫ßy ƒë·ªß.\n",
        "        test_size (float): T·ª∑ l·ªá d·ªØ li·ªáu test (0.0 - 1.0).\n",
        "        seed (int): Seed ƒë·ªÉ chia d·ªØ li·ªáu ng·∫´u nhi√™n.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (train_dataset, test_dataset) - T·∫≠p train v√† test.\n",
        "    \"\"\"\n",
        "    if not (0.0 < test_size < 1.0):\n",
        "        raise ValueError(\"test_size ph·∫£i n·∫±m trong kho·∫£ng (0.0, 1.0)\")\n",
        "    if len(dataset) < 2:\n",
        "        raise ValueError(\"Dataset ph·∫£i c√≥ √≠t nh·∫•t 2 m·∫´u ƒë·ªÉ chia.\")\n",
        "\n",
        "    train_test_split = dataset.train_test_split(test_size=test_size, seed=seed)\n",
        "    print(f\"Chia dataset: {len(train_test_split['train'])} m·∫´u train, {len(train_test_split['test'])} m·∫´u test\")\n",
        "    return train_test_split[\"train\"], train_test_split[\"test\"]\n",
        "\n",
        "# Chia dataset\n",
        "train_dataset, test_dataset = split_dataset(dataset, test_size=0.3)\n",
        "\n",
        "# Ki·ªÉm tra d·ªØ li·ªáu\n",
        "print(\"Train dataset:\", train_dataset)\n",
        "print(\"Test dataset:\", test_dataset)\n",
        "\n",
        "# Truy c·∫≠p m·∫´u c·ª• th·ªÉ\n",
        "sample_train_dataset = train_dataset.select(range(8))  # L·∫•y 10 m·∫´u ƒë·∫ßu ti√™n t·ª´ train\n",
        "sample_test_dataset = test_dataset.select(range(9))    # L·∫•y 10 m·∫´u ƒë·∫ßu ti√™n t·ª´ test\n",
        "\n",
        "print(\"Sample train dataset:\", sample_train_dataset)\n",
        "print(\"Sample test dataset:\", sample_test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCzQIjl_ptbw"
      },
      "source": [
        "# 2. Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bknqLH2piJMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e76fbf25-8c4e-46ab-93cb-a12fb9055545"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# B∆∞·ªõc 2: Chu·∫©n b·ªã tokenizer v√† token h√≥a d·ªØ li·ªáu\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir = \"huggingface\")\n",
        "model = BERTIntentClassification(\n",
        "    model_name=model_name,\n",
        "    num_classes=5\n",
        ")\n",
        "model.freeze_bert() # Froze Layer BERT\n",
        "max_seq_length = 512\n",
        "\n",
        "\n",
        "def collate_fn(features):\n",
        "    inputs = []\n",
        "    labels = []\n",
        "    for element in features:\n",
        "        inputs.append(element.get(\"input_ids\"))\n",
        "        labels.append(element.get(\"label\"))\n",
        "\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    token_inputs = tokenizer(\n",
        "        inputs,\n",
        "        add_special_tokens=True,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=max_seq_length,\n",
        "        return_overflowing_tokens=False,\n",
        "        return_length=False,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    token_inputs.update({\n",
        "        \"labels\": labels,\n",
        "    })\n",
        "    return token_inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZldUk54pj1N"
      },
      "source": [
        "# 3. Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptK7Cy22p2GK"
      },
      "source": [
        "## 3.1 Log Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZkR3vuFp5uN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "815c6207-832b-4d17-e064-4f7277b56c83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qux2ABzMp7Ec"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZRspV7jp8OT"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env\n",
        "load_dotenv()\n",
        "\n",
        "# L·∫•y key t·ª´ bi·∫øn m√¥i tr∆∞·ªùng\n",
        "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
        "print(wandb_api_key[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "215vQ7cOp9oo"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "import os\n",
        "\n",
        "# L·∫•y API key t·ª´ bi·∫øn m√¥i tr∆∞·ªùng v√† ƒëƒÉng nh·∫≠p\n",
        "wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJG_rUYTqwLJ"
      },
      "source": [
        "C√°ch thi·∫øt l·∫≠p th√¥ng qua TrainingArguments\n",
        "Khi s·ª≠ d·ª•ng Trainer, b·∫°n c√≥ th·ªÉ ƒë·∫∑t t√™n d·ª± √°n tr·ª±c ti·∫øp trong TrainingArguments b·∫±ng c√°ch s·ª≠ d·ª•ng tham s·ªë report_to v√† run_name. Tuy nhi√™n, ƒë·ªÉ ƒë·∫∑t project, b·∫°n c·∫ßn kh·ªüi t·∫°o m·ªôt phi√™n wandb tr∆∞·ªõc ho·∫∑c truy·ªÅn c·∫•u h√¨nh n√†y th√¥ng qua wandb.init().\n",
        "\n",
        "ƒêi·ªÅu ch·ªânh TrainingArguments:\n",
        "```python\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_\",          # Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£\n",
        "    eval_strategy=\"epoch\",           # ƒê√°nh gi√° sau m·ªói epoch\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=128,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",            # Th∆∞ m·ª•c l∆∞u log\n",
        "    logging_strategy=\"steps\",        # Log theo steps\n",
        "    logging_steps=10,                # Log sau m·ªói 10 b∆∞·ªõc\n",
        "    save_strategy=\"epoch\",           # L∆∞u checkpoint sau m·ªói epoch\n",
        "    save_total_limit=3,              # L∆∞u t·ªëi ƒëa 3 checkpoint\n",
        "    report_to=\"wandb\",               # B√°o c√°o log t·ªõi wandb\n",
        "    run_name=\"bert_run_1\"            # T√™n phi√™n ch·∫°y tr√™n wandb\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAMgPe3NqAhz"
      },
      "source": [
        "## 3.2 Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3hWcz751mEx"
      },
      "source": [
        "### Ver 1.2.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gs-Z20WNRnE"
      },
      "source": [
        "D∆∞·ªõi ƒë√¢y l√† b·∫£ng t√≥m t·∫Øt chi ti·∫øt c√°ch l∆∞u m√¥ h√¨nh d·ª±a tr√™n chi·∫øn l∆∞·ª£c ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t:\n",
        "\n",
        "| **Lo·∫°i Model**    | **ƒêi·ªÅu Ki·ªán L∆∞u**                                                                 | **Th∆∞ M·ª•c L∆∞u Tr√™n Local**       | **S·ªë L∆∞·ª£ng L∆∞u Tr√™n Local**        | **Th√¥ng Tin Th√™m**                              | **ƒê·ªìng B·ªô L√™n WandB**                  |\n",
        "|--------------------|-----------------------------------------------------------------------------------|-----------------------------------|------------------------------------|-----------------------------------------------|-----------------------------------------|\n",
        "| **Best Model**     | Khi `eval_loss` gi·∫£m                                                             | `output_dir/best_model`           | Ch·ªâ l∆∞u m·ªôt b·∫£n duy nh·∫•t           | L∆∞u th√¥ng tin `epoch` v√† `eval_loss`.          | C√≥: Artifact `best_model`. Th√™m `epoch` v√† `loss` v√†o `metadata`. |\n",
        "| **Final Checkpoint** | Sau m·ªói epoch (checkpoint cu·ªëi c·ªßa epoch)                                        | `output_dir/checkpoint-epoch-<n>` | T·ªëi ƒëa 3 checkpoint g·∫ßn nh·∫•t       | Kh√¥ng c√≥ th√¥ng tin ƒë·∫∑c bi·ªát.                   | Kh√¥ng ƒë·ªìng b·ªô (tr√°nh tr√πng l·∫∑p d·ªØ li·ªáu l·ªõn). |\n",
        "| **Custom Checkpoint** (t√πy ch·ªçn) | Sau m·ªôt s·ªë b∆∞·ªõc c·ªë ƒë·ªãnh ho·∫∑c m·ªëc quan tr·ªçng (n·∫øu c·∫ßn thi·∫øt, v√≠ d·ª•: m·ªói 5 epoch) | T√πy ch·ªânh, v√≠ d·ª•: `output_dir/checkpoint-step-<n>` | Theo √Ω mu·ªën, ho·∫∑c kh√¥ng gi·ªõi h·∫°n | Th√™m c√°c m·ªëc quan tr·ªçng ƒë·ªÉ ph√¢n t√≠ch sau n√†y. | T√πy ch·ªçn (kh√¥ng b·∫Øt bu·ªôc).              |\n",
        "\n",
        "---\n",
        "\n",
        "### **Chi ti·∫øt v·ªÅ b·∫£ng**\n",
        "1. **Best Model**:\n",
        "   - ƒêi·ªÅu ki·ªán: `eval_loss` gi·∫£m.\n",
        "   - Ch·ªâ l∆∞u m·ªôt phi√™n b·∫£n t·ªët nh·∫•t.\n",
        "   - L∆∞u th√¥ng tin epoch v√† loss ƒë·ªÉ d·ªÖ d√†ng tham kh·∫£o ho·∫∑c t·∫£i xu·ªëng sau n√†y.\n",
        "\n",
        "2. **Final Checkpoint**:\n",
        "   - ƒê∆∞·ª£c l∆∞u sau m·ªói epoch.\n",
        "   - Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng checkpoint l∆∞u tr√™n local ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ (v√≠ d·ª•: t·ªëi ƒëa 3 checkpoint).\n",
        "   - Kh√¥ng l∆∞u th√¥ng tin th√™m v√†o checkpoint.\n",
        "\n",
        "3. **Custom Checkpoint** (t√πy ch·ªçn):\n",
        "   - C√≥ th·ªÉ s·ª≠ d·ª•ng n·∫øu b·∫°n mu·ªën l∆∞u checkpoint t·∫°i c√°c m·ªëc th·ªùi gian c·ª• th·ªÉ, ch·∫≥ng h·∫°n nh∆∞ m·ªói 5 epoch ho·∫∑c sau m·ªôt s·ªë b∆∞·ªõc hu·∫•n luy·ªán (steps).\n",
        "   - Th√≠ch h·ª£p khi b·∫°n c·∫ßn ki·ªÉm tra ti·∫øn ƒë·ªô hu·∫•n luy·ªán chi ti·∫øt h∆°n ho·∫∑c mu·ªën l∆∞u backup.\n",
        "\n",
        "---\n",
        "\n",
        "### **T√≥m t·∫Øt logic**\n",
        "- **Best Model**:\n",
        "  - L∆∞u v√†o th∆∞ m·ª•c c·ªë ƒë·ªãnh (`best_model`).\n",
        "  - Ghi ƒë√® khi c√≥ `eval_loss` m·ªõi t·ªët h∆°n.\n",
        "  - ƒê·ªìng b·ªô l√™n WandB.\n",
        "\n",
        "- **Final Checkpoint**:\n",
        "  - L∆∞u sau m·ªói epoch.\n",
        "  - X√≥a checkpoint c≈© nh·∫•t n·∫øu v∆∞·ª£t gi·ªõi h·∫°n `save_total_limit`.\n",
        "  - Kh√¥ng ƒë·ªìng b·ªô l√™n WandB (tr√°nh l√£ng ph√≠ kh√¥ng gian l∆∞u tr·ªØ).\n",
        "\n",
        "- **Custom Checkpoint**:\n",
        "  - T√πy ch·ªçn n·∫øu b·∫°n c·∫ßn l∆∞u th√™m ƒë·ªÉ ph·ª•c v·ª• c√°c m·ª•c ƒë√≠ch c·ª• th·ªÉ.\n",
        "\n",
        "N·∫øu b·∫°n c·∫ßn th√™m b·∫•t k·ª≥ chi ti·∫øt n√†o kh√°c, h√£y cho m√¨nh bi·∫øt nh√©! üòä"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCbkX1KHeT0j"
      },
      "source": [
        "### **B·∫£ng T√≥m T·∫Øt: L∆∞u Best Model v√† Last Model**\n",
        "\n",
        "| **Lo·∫°i Model**    | **Khi N√†o C·∫ßn L∆∞u**                                                                                         | **∆Øu ƒêi·ªÉm**                                                                                       | **H·∫°n Ch·∫ø**                                                                                      |\n",
        "|--------------------|------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|\n",
        "| **Best Model**     | - Khi mu·ªën tri·ªÉn khai m√¥ h√¨nh t·ªët nh·∫•t v·ªõi `eval_loss` th·∫•p nh·∫•t ho·∫∑c `accuracy` cao nh·∫•t.                   | - ƒê·∫£m b·∫£o l∆∞u l·∫°i m√¥ h√¨nh c√≥ hi·ªáu su·∫•t t·ªët nh·∫•t tr√™n t·∫≠p validation.<br>- Ph√π h·ª£p ƒë·ªÉ tri·ªÉn khai.   | - Kh√¥ng l∆∞u tr·∫°ng th√°i ƒë·∫ßy ƒë·ªß (optimizer, scheduler).<br>- Kh√¥ng ti·∫øp t·ª•c hu·∫•n luy·ªán t·ª´ tr·∫°ng th√°i n√†y. |\n",
        "| **Last Model**     | - Khi c·∫ßn ti·∫øp t·ª•c hu·∫•n luy·ªán (fine-tuning) ho·∫∑c kh√¥i ph·ª•c tr·∫°ng th√°i sau khi hu·∫•n luy·ªán k·∫øt th√∫c.         | - L∆∞u ƒë·∫ßy ƒë·ªß tr·∫°ng th√°i (weights, optimizer, scheduler).<br>- Ph√π h·ª£p ƒë·ªÉ ti·∫øp t·ª•c hu·∫•n luy·ªán.    | - C√≥ th·ªÉ kh√¥ng ph·∫£i l√† m√¥ h√¨nh t·ªët nh·∫•t (do overfitting ho·∫∑c underfitting).                     |\n",
        "| **Ch·ªâ L∆∞u Best**   | - Khi ch·ªâ quan t√¢m ƒë·∫øn tri·ªÉn khai m√¥ h√¨nh t·ªët nh·∫•t, kh√¥ng c·∫ßn ti·∫øp t·ª•c hu·∫•n luy·ªán sau n√†y.                  | - Ti·∫øt ki·ªám t√†i nguy√™n l∆∞u tr·ªØ.<br>- T·∫≠p trung v√†o m√¥ h√¨nh t·ªëi ∆∞u cho tri·ªÉn khai.                | - Kh√¥ng th·ªÉ ti·∫øp t·ª•c hu·∫•n luy·ªán n·∫øu c·∫ßn.                                                         |\n",
        "| **Ch·ªâ L∆∞u Last**   | - Khi mu·ªën ƒë·∫£m b·∫£o kh·∫£ nƒÉng kh√¥i ph·ª•c tr·∫°ng th√°i ƒë·ªÉ ti·∫øp t·ª•c hu·∫•n luy·ªán.                                    | - Kh√¥i ph·ª•c ho√†n to√†n qu√° tr√¨nh hu·∫•n luy·ªán.<br>- Ph√π h·ª£p cho fine-tuning ho·∫∑c th·ª≠ nghi·ªám sau n√†y. | - Kh√¥ng ƒë·∫£m b·∫£o ƒë√¢y l√† m√¥ h√¨nh t·ªët nh·∫•t ƒë·ªÉ tri·ªÉn khai.                                           |\n",
        "| **L∆∞u C·∫£ Hai**     | - Khi c·∫ßn c·∫£ tri·ªÉn khai m√¥ h√¨nh t·ªët nh·∫•t v√† ti·∫øp t·ª•c hu·∫•n luy·ªán sau n√†y.                                    | - K·∫øt h·ª£p ∆∞u ƒëi·ªÉm c·ªßa c·∫£ Best Model v√† Last Model.<br>- Linh ho·∫°t trong s·ª≠ d·ª•ng.                 | - T·ªën th√™m t√†i nguy√™n l∆∞u tr·ªØ v√† th·ªùi gian.                                                     |\n",
        "\n",
        "---\n",
        "\n",
        "### **Chi·∫øn L∆∞·ª£c T·ªëi ∆Øu**\n",
        "| **Lo·∫°i L∆∞u** | **T·∫ßn Su·∫•t**                          | **Chi·∫øn L∆∞·ª£c**                                                                                             |\n",
        "|--------------|---------------------------------------|-----------------------------------------------------------------------------------------------------------|\n",
        "| **Best Model** | Khi `eval_loss` gi·∫£m                 | L∆∞u m·ªói l·∫ßn `eval_loss` gi·∫£m ƒë·ªÉ ƒë·∫£m b·∫£o m√¥ h√¨nh t·ªët nh·∫•t lu√¥n ƒë∆∞·ª£c l∆∞u.                                    |\n",
        "| **Last Model** | Sau khi hu·∫•n luy·ªán k·∫øt th√∫c          | L∆∞u tr·∫°ng th√°i cu·ªëi c√πng c·ªßa qu√° tr√¨nh hu·∫•n luy·ªán (weights + optimizer + scheduler).                      |\n",
        "| **K·∫øt h·ª£p**   | Best Model: M·ªói khi `eval_loss` gi·∫£m<br>Last Model: Sau khi k·∫øt th√∫c | L∆∞u c·∫£ Best Model ƒë·ªÉ tri·ªÉn khai v√† Last Model ƒë·ªÉ ti·∫øp t·ª•c hu·∫•n luy·ªán khi c·∫ßn thi·∫øt.                      |\n",
        "\n",
        "---\n",
        "\n",
        "### **L·ª±a Ch·ªçn Ph√π H·ª£p**\n",
        "- **D·ª± √°n tri·ªÉn khai m√¥ h√¨nh nhanh**: L∆∞u **Best Model**.\n",
        "- **D·ª± √°n nghi√™n c·ª©u ho·∫∑c fine-tuning ti·∫øp**: L∆∞u **Last Model**.\n",
        "- **D·ª± √°n quy m√¥ l·ªõn, c·∫ßn c·∫£ tri·ªÉn khai v√† m·ªü r·ªông**: L∆∞u **c·∫£ hai**.\n",
        "\n",
        "H√£y ch·ªçn chi·∫øn l∆∞·ª£c l∆∞u ph√π h·ª£p v·ªõi m·ª•c ti√™u d·ª± √°n c·ªßa b·∫°n! üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLIKjrdaesyG"
      },
      "source": [
        "Thui, ko l∆∞u local n·ªØa, l∆∞u t·∫•t tr√™n wandb ƒëi.\n",
        "- V·ªõi best model: l∆∞u l√™n wandb khi loss gi·∫£m v√† ƒë√£ sau 10 epochs  \n",
        "- V·ªõi last model: l∆∞u l√™n wandb sau m·ªói 10 epochs\n",
        "+, Trong qu√° tr√¨nh l∆∞u th√¨ vi·ªác training v·∫´n di·ªÖn ra Parallel\n",
        "\n",
        "ƒë·ªÅu l∆∞u ƒë·∫ßy ƒë·ªß to√†n b·ªô tham s·ªë ƒë·ªÉ c√≥ th·ªÉ train th√™m t·ª´ c·∫£ ·ªü best model v√† last model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHYLAOxP3Lea"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5n7vK-a9_gM"
      },
      "outputs": [],
      "source": [
        "# class TrainerCustom(Trainer):\n",
        "\n",
        "#     def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "#         \"\"\"\n",
        "#         How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
        "\n",
        "#         Subclass and override for custom behavior.\n",
        "#         \"\"\"\n",
        "#         if \"labels\" in inputs:\n",
        "#             labels = inputs.pop(\"labels\")\n",
        "#         else:\n",
        "#             labels = None\n",
        "\n",
        "#         # S·ª≠ d·ª•ng nn.CrossEntropyLoss() thay v√¨ nn.CrossEntropy\n",
        "#         cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "#         # Ch·∫°y m√¥ h√¨nh v√† nh·∫≠n ƒë·∫ßu ra (logits)\n",
        "#         outputs = model(**inputs)\n",
        "\n",
        "#         # ƒê·∫£m b·∫£o l·∫•y logits t·ª´ outputs (m√¥ h√¨nh tr·∫£ v·ªÅ tuple, l·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n l√† logits)\n",
        "#         logits = outputs\n",
        "\n",
        "#         if labels is None:\n",
        "#             print(\"Labels are None during compute_loss.\")\n",
        "#         if logits is None:\n",
        "#             print(\"Logits are None during compute_loss.\")\n",
        "\n",
        "#         # T√≠nh to√°n loss\n",
        "#         loss = cross_entropy_loss(logits, labels)\n",
        "\n",
        "#         # Tr·∫£ v·ªÅ loss v√† outputs n·∫øu c·∫ßn\n",
        "#         return (loss, outputs) if return_outputs else loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYRbPjlfDSTs"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import wandb\n",
        "\n",
        "# # Kh·ªüi t·∫°o wandb\n",
        "# wandb.init(\n",
        "#     project=\"bert-intent-classification\",  # T√™n d·ª± √°n\n",
        "#     name=\"bert_run_3\"                     # T√™n phi√™n ch·∫°y\n",
        "# )\n",
        "\n",
        "\n",
        "# # B∆∞·ªõc 6: C√†i ƒë·∫∑t tham s·ªë hu·∫•n luy·ªán\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./result__s\",          # Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£\n",
        "#     eval_strategy=\"epoch\",    # ƒê√°nh gi√° sau m·ªói epoch\n",
        "#     learning_rate=2e-4,\n",
        "#     per_device_train_batch_size=128,\n",
        "#     per_device_eval_batch_size=128,\n",
        "#     num_train_epochs=50,\n",
        "#     weight_decay=0.01,\n",
        "#     logging_dir=\"./logs\",\n",
        "#     logging_strategy=\"steps\",\n",
        "#     logging_steps=1,  # Ghi logs m·ªói 500 b∆∞·ªõc hu·∫•n luy·ªán\n",
        "#     save_strategy=\"no\",          # L∆∞u tr·ªçng s·ªë sau m·ªói epoch\n",
        "#     save_total_limit=3,\n",
        "#     label_names = [\"labels\"],\n",
        "#     report_to=\"wandb\",\n",
        "#     run_name=\"bert_run_3\"\n",
        "# )\n",
        "\n",
        "\n",
        "# batch = collate_fn([sample_test_dataset[0]]) # T·∫°o m·ªôt batch t·ª´ m·ªôt m·∫´u ƒë∆°n l·∫ª (sample_test_dataset[0]) ƒë·ªÉ ki·ªÉm tra xem h√†m collate_fn c√≥ ho·∫°t ƒë·ªông ƒë√∫ng kh√¥ng.\n",
        "# print(batch)\n",
        "\n",
        "# # metrics = trainer.evaluate()\n",
        "# # M·ª•c ƒë√≠ch: Ch·∫°y giai ƒëo·∫°n evaluation (ƒë√°nh gi√°) tr√™n eval_dataset (sample_test_dataset) v√† t√≠nh to√°n c√°c metrics nh∆∞:\n",
        "# trainer = TrainerCustom(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=sample_train_dataset,\n",
        "#     eval_dataset=sample_test_dataset,\n",
        "#     tokenizer=tokenizer,\n",
        "#     data_collator=collate_fn,\n",
        "# )\n",
        "\n",
        "# metrics = trainer.evaluate()\n",
        "# print(metrics)  # Ki·ªÉm tra xem c√≥ \"eval_loss\" hay kh√¥ng\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # B∆∞·ªõc 7: T·∫°o Trainer\n",
        "# trainer = TrainerCustom(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=sample_train_dataset,\n",
        "#     eval_dataset=sample_test_dataset,\n",
        "#     tokenizer=tokenizer,\n",
        "#     data_collator = collate_fn,\n",
        "# )\n",
        "\n",
        "# # B∆∞·ªõc 8: Hu·∫•n luy·ªán\n",
        "# trainer.train()\n",
        "\n",
        "# # K·∫øt th√∫c phi√™n wandb\n",
        "# wandb.finish()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vtlo08BaWQhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import wandb\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "class TrainerCustom(Trainer):\n",
        "    def __init__(self, *args, save_every_n_epochs=10, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.best_eval_loss = float(\"inf\")  # Gi√° tr·ªã loss t·ªët nh·∫•t ban ƒë·∫ßu\n",
        "        self.save_every_n_epochs = save_every_n_epochs  # T·∫ßn su·∫•t l∆∞u l√™n WandB\n",
        "        self.best_model_info = {\"epoch\": None, \"loss\": None}\n",
        "        self.last_saved_epoch = 0  # Epoch cu·ªëi c√πng ƒë√£ l∆∞u Best Model v√† Last Model\n",
        "        self.executor = ThreadPoolExecutor(max_workers=2)  # Cho ph√©p t·ªëi ƒëa 2 lu·ªìng song song\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        \"\"\"\n",
        "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
        "\n",
        "        Subclass and override for custom behavior.\n",
        "        \"\"\"\n",
        "        if \"labels\" in inputs:\n",
        "            labels = inputs.pop(\"labels\")\n",
        "        else:\n",
        "            labels = None\n",
        "\n",
        "        # S·ª≠ d·ª•ng nn.CrossEntropyLoss() thay v√¨ nn.CrossEntropy\n",
        "        cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Ch·∫°y m√¥ h√¨nh v√† nh·∫≠n ƒë·∫ßu ra (logits)\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # ƒê·∫£m b·∫£o l·∫•y logits t·ª´ outputs (m√¥ h√¨nh tr·∫£ v·ªÅ tuple, l·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n l√† logits)\n",
        "        logits = outputs\n",
        "\n",
        "        if labels is None:\n",
        "            print(\"Labels are None during compute_loss.\")\n",
        "        if logits is None:\n",
        "            print(\"Logits are None during compute_loss.\")\n",
        "\n",
        "        # T√≠nh to√°n loss\n",
        "        loss = cross_entropy_loss(logits, labels)\n",
        "\n",
        "        # Tr·∫£ v·ªÅ loss v√† outputs n·∫øu c·∫ßn\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    def async_save_model(self, model_dir, artifact_name, metadata=None):\n",
        "        def save():\n",
        "            start_time = time.time()\n",
        "            self.save_model(model_dir)\n",
        "            artifact = wandb.Artifact(artifact_name, type=\"model\")\n",
        "            artifact.add_dir(model_dir)\n",
        "            if metadata:\n",
        "                artifact.metadata = metadata\n",
        "            wandb.log_artifact(artifact)\n",
        "            shutil.rmtree(model_dir, ignore_errors=True)\n",
        "            elapsed_time = time.time() - start_time\n",
        "            print(f\"Model saved and uploaded to WandB: {artifact_name} in {elapsed_time:.2f} seconds\")\n",
        "\n",
        "        self.executor.submit(save)\n",
        "\n",
        "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix: str = \"eval\"):\n",
        "        metrics = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
        "        eval_loss = metrics.get(\"eval_loss\")\n",
        "\n",
        "        # C·∫≠p nh·∫≠t Best Model n·∫øu eval_loss gi·∫£m\n",
        "        if eval_loss is not None and eval_loss < self.best_eval_loss:\n",
        "            print(f\"New best eval_loss: {eval_loss}\")\n",
        "            self.best_eval_loss = eval_loss\n",
        "            self.best_model_info = {\"epoch\": self.state.epoch, \"loss\": eval_loss}\n",
        "\n",
        "            # L∆∞u Best Model l√™n WandB sau m·ªói 10 epochs\n",
        "            if int(self.state.epoch) % self.save_every_n_epochs == 0:\n",
        "                best_model_dir = f\"./tmp_best_model_epoch_{int(self.state.epoch)}\"\n",
        "                artifact_name = f\"best_model_epoch_{int(self.state.epoch)}\"\n",
        "                self.async_save_model(best_model_dir, artifact_name, self.best_model_info)\n",
        "\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def save_last_model(self):\n",
        "        \"\"\"\n",
        "        L∆∞u Last Model l√™n WandB sau m·ªói N epochs.\n",
        "        \"\"\"\n",
        "        if int(self.state.epoch) % self.save_every_n_epochs == 0 and int(self.state.epoch) != self.last_saved_epoch:\n",
        "            print(f\"Saving Last Model at epoch {self.state.epoch} to WandB...\")\n",
        "            last_model_dir = f\"./tmp_last_model_epoch_{int(self.state.epoch)}\"\n",
        "            artifact_name = f\"last_model_epoch_{int(self.state.epoch)}\"\n",
        "            self.async_save_model(last_model_dir, artifact_name)\n",
        "\n",
        "            # C·∫≠p nh·∫≠t epoch cu·ªëi c√πng ƒë√£ l∆∞u\n",
        "            self.last_saved_epoch = int(self.state.epoch)\n",
        "\n",
        "    def train(self, *args, **kwargs):\n",
        "        result = super().train(*args, **kwargs)\n",
        "\n",
        "        # Sau m·ªói epoch, l∆∞u Last Model l√™n WandB\n",
        "        self.save_last_model()\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "# B∆∞·ªõc 6: C√†i ƒë·∫∑t tham s·ªë hu·∫•n luy·ªán\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./result__s\",          # Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£\n",
        "    eval_strategy=\"epoch\",    # ƒê√°nh gi√° sau m·ªói epoch\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=128,\n",
        "    num_train_epochs=50,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=1,  # Ghi logs m·ªói 500 b∆∞·ªõc hu·∫•n luy·ªán\n",
        "    save_strategy=\"no\",          # L∆∞u tr·ªçng s·ªë sau m·ªói epoch\n",
        "    save_total_limit=3,\n",
        "    label_names = [\"labels\"],\n",
        "    report_to=\"wandb\",\n",
        "    run_name=\"bert_run_3\"\n",
        ")\n",
        "\n",
        "\n",
        "import wandb\n",
        "\n",
        "# Kh·ªüi t·∫°o wandb\n",
        "wandb.init(\n",
        "    project=\"bert-intent-classification\",  # T√™n d·ª± √°n\n",
        "    name=\"bert_run_3\"                     # T√™n phi√™n ch·∫°y\n",
        ")\n",
        "\n",
        "\n",
        "trainer = TrainerCustom(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=sample_train_dataset,\n",
        "    eval_dataset=sample_test_dataset,\n",
        "    data_collator=collate_fn,\n",
        "    save_every_n_epochs=10  # L∆∞u Best Model v√† Last Model m·ªói 10 epochs\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "IeWGfPVWVw1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# B∆∞·ªõc 9: ƒê√°nh gi√° tr√™n t·∫≠p ki·ªÉm tra\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "rC3AmIe0T9co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiq5bRFTmzv5"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"What is the weather like today?\"\n",
        "\n",
        "\n",
        "inputs = tokenizer(\n",
        "    sentence,\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "am0boXLrUCXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # ƒê·∫∑t m√¥ h√¨nh ·ªü ch·∫ø ƒë·ªô ƒë√°nh gi√° (kh√¥ng t√≠nh gradient)\n",
        "with torch.no_grad():  # Kh√¥ng c·∫ßn t√≠nh gradient\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predicted_class = torch.argmax(logits, dim=1).item()  # L·∫•y nh√£n d·ª± ƒëo√°n\n",
        "    print(f\"Predicted class: {predicted_class}\")\n"
      ],
      "metadata": {
        "id": "yB4bStFOUDkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTTbVHd551js"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}